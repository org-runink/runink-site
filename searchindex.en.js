var relearn_searchindex = [
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines",
    "content": "📌 Overview Runink is a Go-native distributed pipeline orchestration and governance platform. It defines a self-sufficient, distributed environment to orchestrate and execute data pipelines — replacing complex Kubernetes or Slurm setups with an integrated runtime built on:\nLinux primitives: cgroups, namespaces, exec Go-based execution and scheduling Governance, lineage, and contract enforcement Serverless-style, per-slice isolation and resource control Runink slices run like fast, secure micro-VMs — written in Go, isolated with Linux, coordinated by Raft.\n🔑 Core Principles Go Native \u0026 Linux Primitives: Minimal overhead, high performance Self-Contained Cluster: No external orchestrator needed Serverless Execution Model: Declarative pipelines, smart scheduling Security-First: OIDC, RBAC, secrets, mTLS Data Governance Built-In: Contracts, lineage, auditability Observability: Prometheus-ready, structured logs, golden testing 🚀 Key Features DAG compilation from .dsl + contracts Runi slice execution (lightweight, isolated) Namespace-based Herd isolation Secure secrets vault via Raft Built-in schema contracts and golden testing Metadata lineage and compliance APIs 🧠 System Concepts Runink: DAG compiler from Domain Structured Language (DSL) files and related data contracts. The golang code base to deploy features from configurations files deployed by command actions over the CLI/API. Runi: Lifecycle workload among agents and its available workers, Dag pipeline slice scheduling and execution. A single instance of a pipeline step running as an isolated Runi Slice Process managed by a Runi Agent within the constraints of a specific Herd Herds: Domain boundaries, Secrets Management, gRPC/Rest Services, Runi control plane. A logical grouping construct, similar to a Kubernetes Namespace, enforced via RBAC policies and resource quotas. Provides multi-tenancy and domain isolation. Barn: Metadata governance, lineage tracking A distributed, Raft-backed state store that guarantees strong consistency, high availability, and deterministic orchestration. No split-brain, no guesswork — just fault-tolerant operations. 🛠 Getting Started # Bootstrap your herd runi herd init my-data-herd # Compile DSL → DAG runi compile \\ --scenario features/trade_cdm.dsl \\ --contract contracts/trade_cdm_multi.go \\ --out dags/trade_cdm_dag.go # Generate test data runi synth \\ --scenario features/trade_cdm.dsl \\ --contract contracts/trade_cdm_multi.go \\ --golden cdm_trade_input.json # Run validation runi audit \\ --scenario features/trade_cdm.dsl \\ --contract contracts/trade_cdm_multi.go \\ --golden cdm_trade_input.json # Execute pipeline runi run --dag dags/trade_cdm_dag.go 🧭 Docs \u0026 Exploration 📘 Architecture 🔎 Benchmark Comparison 🧱 Component Overview 📚 Documentation Home 💬 Community Forums 🧪 Project Status Alpha / Experimental — Actively evolving. Feedback-driven development. Architecture represents target vision.\n🤝 Contributing We welcome PRs, issues, and feedback! Start with our contribution guide.\n📜 License Apache 2.0 — View LICENSE on GitHub",
    "description": "📌 Overview Runink is a Go-native distributed pipeline orchestration and governance platform. It defines a self-sufficient, distributed environment to orchestrate and execute data pipelines — replacing complex Kubernetes or Slurm setups with an integrated runtime built on:\nLinux primitives: cgroups, namespaces, exec Go-based execution and scheduling Governance, lineage, and contract enforcement Serverless-style, per-slice isolation and resource control Runink slices run like fast, secure micro-VMs — written in Go, isolated with Linux, coordinated by Raft.",
    "tags": [],
    "title": "Runink Docs",
    "uri": "/runink-site/docs/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Runink is a self-contained, distributed data orchestration environment — purpose-built to run secure, declarative data pipelines without unnecessary extra components.\nWritten in Go, hardened by Linux namespaces and cgroups, and coordinated with Raft, Runink:\nUses Go slices instead of containers Wraps execution with kernel primitives (namespaces, cgroups, pipes) Orchestrates them with Raft-consistent DAGs Tracks everything via native Governance/Observability layers Runink is ideal for teams that need:\n🔐 Governance (lineage, contracts, access control) ⚡ Performance (compiled pipelines, minimal runtime) 🧪 Testing (golden files, scenario coverage) 🧱 Isolation (multi-tenant slices within herds) 🧠 Who is it for? For Engineers For Organizations ✅ Run DAGs with runi CLI ✅ Enforce data contracts across teams ✅ Version features with Git ✅ Prove lineage, compliance \u0026 SLOs 🧰 How it Works Each pipeline is defined with:\nDSL Scenario Files → declarative @step, @source, @sink specs Contracts → Go structs with schema and validation logic Herds → Resource-scoped namespaces for pipeline execution Slices → Isolated processes launched under cgroups \u0026 namespaces Runink compiles DSL + contracts into DAGs, then schedules them on bare metal, cloud VMs, or edge workers — no orchestrator required.\n🔐 Built for Data Governance Runink includes:\n✅ RBAC \u0026 OIDC Integration ✅ Schema Contracts with Versioning ✅ Lineage Tracking by Run, Herd, Contract ✅ Golden Test Framework for Pipeline Scenarios ✅ Secure Secrets Delivery via Raft-backed store 🖼 Architecture Diagram 🛠 Get Involved 📚 Documentation 🤝 Contributing 🧠 Architecture Overview 🔍 Benchmark Comparison ⚠️ Development Status Runink is currently in alpha and under active development. Feedback and contributions are welcome.\nView on GitHub →",
    "description": "Runink is a Go-native platform for orchestrating secure, testable, and governance-driven data pipelines at scale.",
    "tags": [],
    "title": "Runink: Declarative, Secure Data Pipelines",
    "uri": "/runink-site/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Runink Architecture: Go/Linux Native Distributed Data Environment Self-sufficient, distributed environment for orchestrating and executing data pipelines using Go and Linux primitives. This system acts as the cluster resource manager and scheduler (replacing Slurm), provides Kubernetes-like logical isolation and RBAC, integrates data governance features, and ensures robust security and observability. It aims for high efficiency by avoiding traditional virtualization or container runtimes like Docker. Define a self-sufficient, distributed environment for orchestrating and executing data pipelines using Go and Linux primitives, with enhanced metadata capabilities designed to support standard data governance (lineage, catalog) AND future integration of LLM-generated annotations.\nHigh-Level Architecture Runink operates with a Control Plane managing multiple Worker Nodes, each running a Runi Agent.\n%%{init:{\"fontFamily\":\"monospace\", \"sequence\":{\"showSequenceNumbers\":true}}}%% flowchart LR subgraph External [\"External Interaction\"] A[\"User / Client (CLI, UI, API)\"] end subgraph ControlPlane [\"HERD CONTROL PLANE (Services state managed in Barn via Raft)\"] direction TB B[\"API Server (Gateway, AuthN/Z)\"] subgraph StateStore [\"Cluster State Store (Barn)\"] D[\"Barn (KV Store)\"] subgraph RaftConsensus [\"Raft Protocol (HA/Consistency)\"] D1[\"Leader\"] --- D2[\"Followers\"] end D --- RaftConsensus end subgraph InternalServices [\"Internal Services (Rely on Barn)\"] direction TB C[\"(Identity \u0026 RBAC Mgr)\"] E[\"Scheduler\"] F[\"Secrets Manager\"] G[\"Data Governance Svc\"] end %% Interaction Flow: User -\u003e API -\u003e Barn \u0026 Services -\u003e Barn A --\u003e B B --\u003e|API Server validates state in Barn / AuthZ| D B --\u003e|API Server Coordinates with RBAC Mgr| C B --\u003e|API Server Coordinates with Scheduler| E B --\u003e|API Server Coordinates with Secrets Mgr| F B --\u003e|API Server Coordinates with Governance Svc| G %% Services use Barn for persistence and state synchronization C --\u003e|Stores/Reads Policies| D E --\u003e|Watches Tasks / Stores Placements| D F --\u003e|Stores/Reads Secrets| D G --\u003e|Stores/Reads Core Metadata \u0026 Lineage Refs| D end subgraph WorkerNode [\"Worker Node\"] direction TB H[\"Runi Agent\"] I[\"Runi Slice\"] H --\u003e| Agent manages Slice | I end subgraph ExternalServices [\"External Services\"] J[\"External Services (DB, Object Store, LLMs)\"] end %% Control Plane -\u003e Worker Interactions (Orchestrated) E --\u003e|Instructs Agent to Place Task| H F --\u003e|Delivers Secrets via Agent| H %% Worker -\u003e Control Plane / External Interactions I --\u003e|Slice Reports Lineage/Metadata to Governance Service| G I --\u003e|Slice interacts with external systems over Data/APIs| J %% Styling Definitions (Adjusted for better light/dark theme visibility) style External fill:#aaccff,stroke:#004488,stroke-width:2px,color:#000 style ControlPlane fill:#e0e0e0,stroke:#444444,stroke-width:2px,color:#000 style StateStore fill:#f0f0e8,stroke:#777777,stroke-width:1px,color:#000 %% Renamed from StateAndConsensus style RaftConsensus fill:#ffebcc,stroke:#aa7700,stroke-width:1px,stroke-dasharray: 5 5,color:#000 style WorkerNode fill:#cceece,stroke:#006600,stroke-width:2px,color:#000 style ExternalServices fill:#ffcccc,stroke:#990000,stroke-width:2px,color:#000 Runi Agent / Worker slice channels orchesrtration Ephemeral UIDs \u0026 mTLS Each slice runs as:\nA non-root ephemeral user With an Herd-specific UID/GID In an isolated namespace Authenticated over mTLS via service tokens %%{init:{\"fontFamily\":\"monospace\", \"sequence\":{\"showSequenceNumbers\":true}}}%% flowchart TD A[\"Runi Agent\u003cbr/\u003e(PID 1 inside namespace)\"] --\u003e B[\"Launch Slice\u003cbr/\u003e(Normalize step)\"] B --\u003e C1[\"io.Pipe() Reader\u003cbr/\u003e→ Goroutine: Validate step\"] B --\u003e C2[\"io.Pipe() Reader\u003cbr/\u003e→ Goroutine: Enrich step\"] B --\u003e D[\"Final Writer\u003cbr/\u003e(sink to disk or message bus)\"] Core Principles User Interaction: Client requests are often scoped to a specific Herd. API Server / RBAC: Enforces RBAC policies based on user/service account permissions within a target Herd. Cluster State Store: Explicitly stores Herd definitions and their associated resource quotas. Scheduler: Considers Herd-level quotas when making placement decisions. Secrets Manager: Access to secrets might be scoped by Herd. Data Governance: Metadata (lineage, annotations) can be tagged by or associated with the Herd it belongs to. Runi Agent: Receives the target Herd context when launching a slice and uses this information to potentially configure namespaces and apply appropriate cgroup limits based on Herd quotas. Runi Slice: A single instance of a pipeline step running as an isolated Worker Slice Process. Executes entirely within the logical boundary and resource constraints defined by its assigned Herd. Runi Pipes: Primarily used now for internal communication within the Runi Agent to capture logs/stdio from the Runi Slice it execs, rather than for primary data transfer between steps. Herd: A logical grouping construct, similar to a Kubernetes Namespace, enforced via RBAC policies and potentially mapped to specific sets of Linux namespaces managed by Agents. Provides multi-tenancy and team isolation. Quotas can be applied per Herd. Go Native \u0026 Linux Primitives: Core components written in Go, directly leveraging cgroups, namespaces (user, pid, net, mount, uts, ipc), pipes, sockets, and exec for execution and isolation. Self-Contained Cluster Management: Manages a pool of physical or virtual machines, schedules workloads onto them, and handles node lifecycle. Serverless Execution Model: Users define pipelines and resource requests; Runink manages node allocation, scheduling, isolation, scaling (by launching more slices), and lifecycle. Users are subject to quotas managed via cgroups. Security First: Integrated identity (OIDC), RBAC, secrets management, network policies, encryption in transit/rest. Data Governance Aware: Built-in metadata tracking, lineage capture, and support for quality checks. With extension for storage/management of rich data annotations (e.g., from LLMs). Rich Observability: Native support for metrics (Prometheus) and logs (Fluentd). Runink’s Execution Overview Runink executes data pipelines using Go “slices” — lightweight, isolated execution units designed to model both bounded (batch) and unbounded (streaming) data workloads. These are:\nSpawned by the Runi agent on worker nodes Executed as isolated processes Scoped to Herd namespaces Constrained by cgroups Communicate via pipes, sockets, or gRPC streams This orchestration is Raft-coordinated, making every launch deterministic, fault-tolerant, and observable.\n🧬 What Are Bounded and Unbounded Slices? Type Use Case Description Bounded Batch ETL, contract validation Processes a finite dataset and terminates Unbounded Streaming ingestion, log/event flows Long-running, backpressured pipelines with checkpointing Both types run as Go processes within a controlled Herd namespace, and can be composed together in DAGs.\n🧰 Slice Internals: Go + Linux Synergy Each slice is a native Go process managed via:\n✅ Cgroups Applied per Herd, per slice Limits on CPU, memory, I/O Enforced using Linux cgroupv2 hierarchy Supports slice preemption and fair resource sharing ✅ Namespaces User, mount, network, and PID namespaces Enforce isolation between tenants (Herds) Prevent noisy-neighbor problems and info leaks ✅ Pipes \u0026 IPC Use of os.Pipe() or io.Pipe() in Go to model stage-to-stage communication net.Pipe() and UNIX domain sockets for local transport Optionally enhanced via io.Reader, bufio, or gRPC streams (for cross-node slices) ✅ Execution os/exec with setns(2) and clone(2) to launch each slice Environment-injected config and secrets fetched securely via Raft-backed Secrets Manager 🔄 Raft as Execution Backbone The Barn (Cluster State Store) is Raft-backed. It ensures:\nRaft Role Benefit to Runink Leader Election Prevents race conditions in pipeline launches Log Replication Guarantees all agents/schedulers share same DAG, lineage, and config Strong Consistency Execution decisions are deterministic and audit-traceable Fault Tolerance Node crashes do not corrupt state or duplicate work Examples of Raft-integrated flows:\nDAG submission is a Raft log entry Herd quota changes update slice scheduling state DLQ routing is replicated for contract validation violations Slice termination is consensus-driven (no orphaned processes) Programming Approaches: Why They Power Runink Runink’s architecture isn’t just Go-native — it’s intentionally designed around a few low-level but high-impact programming paradigms. These concepts are what let Runink outperform containerized stacks, enforce security without overhead, and keep pipelines testable, composable, and fast. Runink takes a radically different approach to pipeline execution than traditional data platforms — instead of running heavy containers, JVMs, or external orchestrators, Runink uses Go-native workers, Linux primitives like cgroups and namespaces, and concepts like data-oriented design and zero-copy streaming to deliver blazing-fast, memory-stable, and secure slices.\nBelow, we walk through the four core techniques and where they show up in Runink’s components.\n🔄 1. Functional Pipelines “Like talking how your data flow over high-level functions.”\nRunink’s .dsl compiles to Go transforms that behave like pure functions: they take input (usually via io.Reader), apply a deterministic transformation, and emit output (via io.Writer). There’s no shared mutable state, no side effects — just clear dataflow.\nThis makes pipelines:\nComposable: steps can be reused across domains Testable: golden tests assert input/output correctness Deterministic: behavior doesn’t depend on cluster state ✅ Why it matters: It brings unit testability and DAG clarity to data pipelines — without needing a centralized scheduler or stateful orchestrator.\n2. Data-Oriented Design (DOD) “Design for the CPU, not the developer.”\nInstead of modeling data as deeply nested structs or objects, Runink favors flat, contiguous Go structs. This aligns memory layout with CPU cache lines and avoids heap thrashing. This is especially important for Runink’s slice execution and contract validation stages, where predictable access to batches of structs (records) matters.\nContracts are validated by scanning []struct batches in tight loops. Pointers and indirection are minimized for GC performance. Contracts power both validation and golden test generation. Use slices of structs over slices of pointers to enable CPU cache locality. Align field access with columnar memory usage if streaming transforms run across many rows. Preallocate buffers in Runi Agent’s slice execution path to avoid GC churn. Core Idea: Layout memory for how it will be accessed, not how it’s logically grouped. Runink’s slices often scan, validate, or enrich large batches of records — so struct layout, batching, and memory predictability directly impact performance.\nApply DOD in Runink: Prefer flat structs over nested ones in contracts: // Better type User struct { ID string Name string Email string } // Avoid: nested fields unless necessary type User struct { Meta struct { ID string } Profile struct { Name string Email string } } In runi slice: Use sync.Pool for reusable buffers (especially JSON decoding). Pre-size buffers based on contract hints (e.g., maxRecords=10000). Avoid interface{} — use generated structs via go/types or go:generate. Benefits: Better memory throughput, fewer allocations, and Go GC-friendliness under load. Use structs of arrays (SoA) or []User with preallocated slices in transformations. Minimize pointer indirection. Use value receivers and avoid *string, *int unless you need nil. Design transforms that operate in tight loops, e.g., for _, rec := range batch. Go structs are faster to iterate over than Python dictionaries or Java POJOs. Access patterns align with how CPUs fetch and cache data. Contract validation and transforms run over preallocated []struct batches, not heap-bound objects. 💡 For Python/Java devs: Think of this like switching from dicts of dicts to NumPy-like flat arrays — but in Go, with static types and no GC spikes.\n✅ Why it matters: You get predictable memory use and cache-friendly validation at slice scale — perfect for CPU-bound ETL or large-batch processing.\n3. Zero-Copy and Streaming Pipelines “Avoid full in-memory materialization — process as the data flows.”\nInstead of []record → transform → []record, Runink pipelines follow stream → transform → stream — minimizing allocations and maximizing throughput. Avoid unnecessary data marshaling or full deserialization. Rely on:\nTransforms consume from io.Reader and emit to io.Writer. Stages communicate via os.Pipe(), net.Pipe(), or chan Record for intra-slice streaming. Only materialize records when needed for validation or transformation. Intermediate results never fully materialize in memory. Core Idea: Instead of []Record -\u003e Transform -\u003e []Record, operate on streams of bytes or structs using io.Reader, chan Record, or even UNIX pipes between stages.\nRunink Optimizations: Use io.Reader → Decoder → Transform → Encoder → io.Writer chain.\nDesign step transforms like this:\nfunc ValidateUser(r io.Reader, w io.Writer) error { decoder := json.NewDecoder(r) encoder := json.NewEncoder(w) for decoder.More() { var user contracts.User if err := decoder.Decode(\u0026user); err != nil { return err } if isValid(user) { encoder.Encode(user) } } return nil } For multi-stage slices, use os.Pipe():\nr1, w1 := os.Pipe() r2, w2 := os.Pipe() go Normalize(r0, w1) // input -\u003e step 1 go Enrich(r1, w2) // step 1 -\u003e step 2 go Sink(r2, out) // step 2 -\u003e sink Benefits: Constant memory even for massive datasets. Backpressure: If downstream slows down, upstream blocks — great for streaming (Kafka, etc.). Enables DLQ teeing: tee := io.MultiWriter(validOut, dlqSink). Uses io.Reader / io.Writer rather than buffering everything in memory. Transforms run as pipes between goroutines — like UNIX but typed. Memory stays flat, predictable, and bounded — even for 10M+ record streams. 💡 For pandas/Spark devs: This is closer to generator pipelines or structured stream micro-batches, but with Go’s backpressure-aware channels and streaming codecs.\n✅ Why it matters: You can process unbounded streams or 100GB batch files with a stable memory footprint — and gain built-in backpressure and DLQ support.\n4. Declarative Scheduling with Constraint Propagation “Schedule via logic, not instructions.” The Herd and Runi Agent coordination already benefits from Raft-backed state, but push it further with affinity-aware, declarative scheduling:\nRunink doesn’t assign slices imperatively. It solves where to run things, based on:\nIsolation: @herd(\"analytics\") Define resource constraints (e.g., @requires(cpu=2, memory=512Mi, label=“gpu”)) in .dsl. Placement: @affinity(colocate_with=\"step:Join\") Propagate slice placement decisions through constraint-solving logic instead of imperative scheduling. Record constraints in the Raft-backed state store to enforce deterministic task placement. You can build this as a small DSL-on-DSL layer (e.g. @affinity(domain=\"finance\", colocate_with=\"step:JoinUsers\")).\nBenefit: Stronger determinism, replayability, and multi-tenant safety.\nCore Idea: Model placement as a set of constraints: affinity, herd quota, GPU needs, tenant isolation, etc. Let the scheduler solve the placement rather than being told where to run.\nRunink DSL Extension: In .dsl:\n@step(\"RunLLMValidation\") @affinity(label=\"gpu\", colocate_with=\"step:ParsePDFs\") @requires(cpu=\"4\", memory=\"2Gi\") This can be compiled into metadata stored in the Raft-backed scheduler store.\nScheduler Logic (Pseudo-Go): type Constraints struct { CPU int Memory int Affinity string Colocate string HerdScope string } func ScheduleStep(stepID string, constraints Constraints) (NodeID, error) { candidates := filterByHerd(constraints.HerdScope) candidates = filterByResources(candidates, constraints.CPU, constraints.Memory) if constraints.Colocate != \"\" { candidates = colocateWith(candidates, constraints.Colocate) } if constraints.Affinity != \"\" { candidates = matchLabel(candidates, constraints.Affinity) } return pickBest(candidates) } These constraints are stored in the Raft-backed Barn and evaluated by the scheduler. In this sense, all decisions are Raft-logged, making slice scheduling auditable and replayable.\n💡 If you’re used to Kubernetes or Docker: Think of slices as ephemeral containers, but 10x faster — no image pulls, no pod scheduling latency. No containers, no clusters — just data pipelines that behave like code.\n✅ Why it matters: Runink achieves multi-tenant safety, fault-tolerant execution, and reproducible placement — without complex K8s YAML or retries.\nSummary Table Approach Use In Runink Why It Powers Runink Functional Pipelines .dsl → Go transforms via @step() Clear transforms, reusable logic, golden testing Data-Oriented Design Contract enforcement, slice internals Memory locality, low-GC, CPU-efficient pipelines Zero-Copy Streaming Slice-to-slice transport, pipe-to-pipe steps Constant memory, streaming support, low latency Declarative Scheduling Herd quotas + slice placement affinity in .dsl raft store Deterministic, fair, replayable orchestration Future LLM Integration Pipeline Definition: A user defines a pipeline step specifically for LLM annotation. This step specifies the input data source (e.g., path on shared FS/MinIO), the target LLM (e.g., OpenAI model name or internal service endpoint), the prompt, and potentially the output format. Scheduling: The Scheduler assigns this step to a Runi Agent. If targeting an internal LLM requiring specific hardware (GPU), the scheduler uses node resource information (reported by Agents) for placement. Execution: The Runi Agent launches a Worker Slice Process for this step. Credentials: The Worker Slice receives necessary credentials (e.g., OpenAI API key, MinIO access key) securely via the Secrets Manager. LLM Call: The worker reads input data, constructs the prompt, calls the relevant LLM API (external or internal), potentially handling batching or retries. Metadata Persistence: Upon receiving results, the worker extracts the annotations, formats them according to the Data Governance Service schema, and sends them via gRPC call to the service, linking them to the input data reference. It also reports standard lineage (input data -\u003e LLM step -\u003e annotations). Usage: Downstream pipeline steps or external users can then query the Data Governance Service (via API Server) to retrieve these annotations for further processing, reporting, or analysis.",
    "description": "Runink Architecture: Go/Linux Native Distributed Data Environment Self-sufficient, distributed environment for orchestrating and executing data pipelines using Go and Linux primitives. This system acts as the cluster resource manager and scheduler (replacing Slurm), provides Kubernetes-like logical isolation and RBAC, integrates data governance features, and ensures robust security and observability. It aims for high efficiency by avoiding traditional virtualization or container runtimes like Docker. Define a self-sufficient, distributed environment for orchestrating and executing data pipelines using Go and Linux primitives, with enhanced metadata capabilities designed to support standard data governance (lineage, catalog) AND future integration of LLM-generated annotations.",
    "tags": [],
    "title": "Architecture",
    "uri": "/runink-site/docs/architecture/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Runink Platform Components This page describes the core building blocks of Runink — from the API server to slices — that make up the distributed data environment. Each component serves a purpose in ensuring secure, auditable, and high-performance pipeline execution.\nComponents Table Component Role Location API Server Entry point, AuthN/Z, coordination Control Plane Identity Manager OIDC/JWT validation and RBAC enforcement Control Plane Barn Raft-backed KV store Control Plane Scheduler DAG-aware placement engine Control Plane Secrets Manager Encrypted secret storage and delivery Control Plane Governance Svc Lineage, quality, LLM annotations Control Plane Runi Agent Worker orchestrator (cgroup+namespace) Worker Node Runi Slice Executed unit of pipeline logic Worker Node Herd Tenant boundary and resource isolation System-wide Contracts Data validation and schema enforcement Contracts repo DSL Parser Converts .dsl to Go DAGs Build pipeline Runink Services 📘 Contract Engine All data contracts (schemas) are defined in Go structs, with support for:\nRequired/optional fields Type validation and coercion Golden testing and schema diffing Metadata annotations (e.g., PII, lineage tags) 💼 Used in: DSL @contract, golden tests, slice validation stages.\n✍️ Feature DSL Parser Parser and compiler for .dsl files.\nConverts scenario definitions into Go-based DAGs Enforces step ordering and contract compliance Attaches metadata for scheduling, RBAC, lineage 🔤 Keywords: @step, @contract, @source, @sink, @affinity, @requires.\n🏃 Runi Agent Daemon running on each worker node, responsible for execution.\nRegisters with the control plane. Launches slices as Go processes within cgroups and namespaces. Sets up stdio pipes, config injection, and secrets access. Collects logs and exposes Prometheus metrics. 💡 Design: PID 1 in isolated namespace, manages ephemeral slices securely.\n⚙️ Runi Slice A single unit of work — a pipeline step — running in an isolated environment.\nExecuted via os.Exec as a native Go binary. Enforces herd-defined resource quotas using cgroups. Receives config, secrets, and contracts. Reports lineage to Governance Service. 📦 Properties: Ephemeral, scoped, observable, auditable.\n🧱 Barn (Cluster State Store) A Raft-backed KV store providing durable, consistent cluster state.\nStores pipeline definitions, slice metadata, herd configs, secrets, etc. Supports leader election and quorum for all control plane decisions. 🛡️ Guarantees: High availability, deterministic orchestration, and strong consistency.\n🧰 Herd Logical boundary for multi-tenancy, quotas, and governance.\nMaps to a namespace (network, user, mount, etc.). RBAC is scoped per herd. Resource quotas applied at the herd level. All metadata, secrets, and lineage are tagged with a herd context. 🔐 Analogy: Like Kubernetes namespaces but tighter and more secure.\nHerd Control Plane Services 📡 API Server The entry point for all client interactions (CLI, UI, and service integrations).\nExposes REST/gRPC APIs secured via OIDC/JWT. Enforces RBAC and herd scoping. Forwards validated requests to: State store (Barn) Identity Manager Scheduler Secrets Manager Governance Service 🔐 Security: Applies policies based on identity and herd-level permissions.\n🧠 Identity \u0026 RBAC Manager Responsible for identity resolution and access control.\nValidates JWT/OIDC tokens. Resolves user roles and herd membership. Provides per-herd scoped RBAC policies. 📘 Location: Can run co-located with the API server or standalone.\n📅 Slice Scheduler The component responsible for task placement and orchestration.\nReads resource constraints from DSLs (@requires). Evaluates herd quotas, affinities, and node health. Determines optimal slice placement. Writes placements into Barn. 🧮 Logic: Constraint-solving over stateful inputs — affinity, quotas, node availability.\n🔐 Secrets Manager Handles secure secrets storage and delivery.\nStores secrets in encrypted form (AES/GCM). Enforces access via RBAC. Slices receive secrets via Runi Agent during launch. 🗝️ Design: Secrets access scoped by herd and role, logged via Raft.\n📊 Data Governance Service Tracks lineage, metadata, and annotations for all slices.\nStores rich metadata per run, stage, and contract hash. Supports querying for audit, compliance, and debugging. Can receive LLM-based annotations. 🔎 Outputs: Lineage graphs, quality reports, field-level annotations.\n🔍 Observability Stack Built-in support for:\nPrometheus: Metrics exposure via /metrics Fluentd or stdout logs: Structured JSON logs captured per slice gRPC metadata reporting: Trace context, tags, and result metadata 🧭 Goal: Enable deep pipeline inspection without needing external agents.\n🔄 Pipes and Channels Slices and agents use pipes (via io.Pipe, os.Pipe, net.Pipe) to transmit data and logs.\nSteps within a slice communicate via in-memory streams. No intermediate buffering — zero-copy, backpressure-safe. Logs are captured via stdout/stderr and piped to the agent. 🚰 Benefits: Stream processing, constant memory, no containers.",
    "description": "Runink Platform Components This page describes the core building blocks of Runink — from the API server to slices — that make up the distributed data environment. Each component serves a purpose in ensuring secure, auditable, and high-performance pipeline execution.\nComponents Table Component Role Location API Server Entry point, AuthN/Z, coordination Control Plane Identity Manager OIDC/JWT validation and RBAC enforcement Control Plane Barn Raft-backed KV store Control Plane Scheduler DAG-aware placement engine Control Plane Secrets Manager Encrypted secret storage and delivery Control Plane Governance Svc Lineage, quality, LLM annotations Control Plane Runi Agent Worker orchestrator (cgroup+namespace) Worker Node Runi Slice Executed unit of pipeline logic Worker Node Herd Tenant boundary and resource isolation System-wide Contracts Data validation and schema enforcement Contracts repo DSL Parser Converts .",
    "tags": [],
    "title": "Components",
    "uri": "/runink-site/docs/components/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "1. Architecture \u0026 Paradigm Runink: A Go/Linux-native, vertically integrated data platform that combines execution, scheduling, governance, and observability in a single runtime. Unlike traditional stacks, Runink does not rely on Kubernetes or external orchestrators. Instead, it uses a Raft-based control plane to ensure high availability and consensus across services like scheduling, metadata, and security — forming a distributed operating model purpose-built for data.\nCompetitors: Use a layered, loosely coupled stack:\nExecution: Spark, Beam (JVM-based) Orchestration: Airflow, Dagster (often on Kubernetes) Transformation: DBT (runs SQL on external data platforms) Cluster Management: Kubernetes, Slurm Governance: Collibra, Apache Atlas (external) Key Differentiator: Runink is built from the ground up as a distributed system — with Raft consensus at its core — whereas competitors compose multiple tools that communicate asynchronously or rely on external state systems.\n1. MapReduce vs. RDD vs. Raft for Data Pipelines 1.1 Architecture \u0026 Paradigm Aspect MapReduce RDD Raft (Runink model) Origin Google (2004) Spark (2010) Raft (2013) adapted for distributed control Execution Model Batch, two-stage (Map → Reduce) In-memory DAGs of transformations Real-time coordination of distributed nodes Consistency Model Eventual (job outputs persisted) Best effort (job outputs in memory, lineage for recovery) Strong consistency (N/2+1 consensus) Primary Use Large batch analytics Interactive, iterative analytics Distributed metadata/state management for pipelines Fault Tolerance Output checkpointing Lineage-based recomputation Log replication and state machine replication 1.2. Performance \u0026 Efficiency Aspect MapReduce RDD Raft (Runink) Cold Start Time High (JVM startup, slot allocation) Medium (Spark cluster overhead) Low (Go processes, native scheduling) Memory Use Disk-heavy Memory-heavy (RDD caching) Lightweight (control metadata, not bulk data) I/O Overhead Heavy disk I/O (HDFS reads/writes) Network/memory optimized, but needs enough RAM Minimal (only metadata replication) Pipeline Complexity Requires multiple jobs for DAGs Natural DAG execution Direct DAG compilation from DSLs (Runink) 1.3. Data Governance and Lineage Aspect MapReduce RDD Raft (Runink) Built-in Lineage No (external) Yes (RDD lineage graph) Yes (atomic commit of contracts, steps, runs) Governance APIs Manual (logs, job output) Partial (Spark listeners) Native (contracts, lineage logs, per-slice metadata) Auditability Hard to reconstruct Possible with effort Native per-run audit logs, Raft-signed events 1.4. Fault Tolerance and Recovery Aspect MapReduce RDD Raft (Runink) Recovery Mechanism Re-run failed jobs Recompute from lineage Replay committed log entries Failure Impact Full-stage re-execution Depends on lost partitions Minimal if quorum is maintained Availability Guarantee None Partial (driver failure = job loss) Strong (as long as majority nodes are alive) 1.5. Security and Isolation Aspect MapReduce RDD Raft (Runink) Authentication Optional Optional Mandatory (OIDC, RBAC) Secrets Management Ad hoc Ad hoc Native, Raft-backed, scoped by Herds Multi-Tenancy None None Herd isolation (namespace + cgroup enforcement) 1.6 Real case scenario example Imagine a critical pipeline for trade settlement:\nMapReduce would force every job to write to disk between stages — slow and painful for debugging. RDD would speed things up but require heavy RAM and still risk full job loss if the driver fails. Raft (Runink) keeps every contract, every transformation, every secret atomically committed and recoverable — even if a node crashes mid-run, the system can resume from the last committed stage safely. 2. Raft Advantages for Distributed Coordination Runink:\nUses Raft for strong consistency and leader election across:\nControl plane state (Herds, pipelines, RBAC, quotas) Scheduler decisions Secrets and metadata governance Guarantees:\nNo split-brain conditions Predictable and deterministic behavior in failure scenarios Fault-tolerant HA (N/2+1 consensus) Competitors:\nKubernetes uses etcd (Raft-backed), but tools like Airflow/Spark have no equivalent. Scheduling decisions, lineage, and metadata handling are often eventually consistent or stored in external systems without consensus guarantees. Result: higher complexity, latency, and coordination failure risks under scale or failure. 3. Performance \u0026 Resource Efficiency Runink:\nWritten in Go for low-latency cold starts and efficient concurrency. Uses direct exec, cgroups, and namespaces, not Docker/K8s layers. Raft ensures low-overhead coordination, avoiding polling retries and state divergence. Competitors:\nSpark is JVM-based; powerful but resource-heavy. K8s introduces orchestration latency, plus pod startup and scheduling delays. Airflow relies on Celery/K8s executors with less efficient scheduling granularity. 4. Scheduling \u0026 Resource Management Runink:\nCustom, Raft-backed Scheduler matches pipeline steps to nodes in real time. Considers Herd quotas, CPU/GPU/Memory availability. Deterministic task placement and retry logic are logged and replayable via Raft. Competitors:\nKubernetes schedulers are general-purpose and not pipeline-aware. Airflow does not control actual compute — delegates to backends like K8s. Slurm excels in HPC, but lacks pipeline-native orchestration and data governance. 5. Security Model Runink:\nSecure-by-default with OIDC + JWT, RBAC, Secrets Manager, mTLS, and field-level masking. Secrets are versioned and replicated with Raft, avoiding plaintext spillage or inconsistent states. Namespace isolation per Herd. Competitors:\nKubernetes offers RBAC and secrets, but complexity leads to misconfigurations. Airflow often shares sensitive configs (connections, variables) across DAGs. 6. Data Governance, Lineage \u0026 Metadata Runink:\nBuilt-in Data Governance Service stores contracts, lineage, quality metrics, and annotations. Changes are committed to Raft, ensuring atomic updates and rollback support. Contracts and pipeline steps are versioned and tracked centrally. Competitors:\nRequire integrating platforms like Atlas or Collibra. Lineage capture is manual or partial, with data loss possible on failure or drift. Metadata syncing lacks consistency guarantees. 7. Multi-Tenancy Runink:\nUses Herds as isolation units — enforced via RBAC, ephemeral UIDs, cgroups, and namespace boundaries. Raft ensures configuration updates (quotas, roles) are safely committed across all replicas. Competitors:\nKubernetes uses namespaces and resource quotas. Airflow has no robust multi-tenancy — teams often need separate deployments. 8. LLM Integration \u0026 Metadata Handling Runink:\nLLM inference is a first-class pipeline step. Annotations are tied to lineage and stored transactionally in the Raft-backed governance store. Competitors:\nLLMs are orchestrated as container steps via KubernetesPodOperator or Argo. Metadata is stored in external tools or left untracked. 9. Observability Runink:\nBuilt-in metrics via Prometheus, structured logs via Fluentd. Metadata and run stats are Raft-consistent, enabling reproducible audit trails. Observability spans from node → slice → Herd → run. Competitors:\nSpark, Airflow, and K8s use external stacks (Loki, Grafana, EFK) that need configuration and instrumentation. Logs may be disjointed or context-lacking. 10. Ecosystem \u0026 Maturity Runink:\nEarly-stage, but intentionally narrow in scope and highly integrated. No need for external orchestrators or data governance platforms. Competitors:\nVast ecosystems (Airflow, Spark, DBT, K8s) with huge community support. Tradeoff: Requires significant integration, coordination, and DevOps effort. 11. Complexity \u0026 Operational Effort Runink:\nHigh initial build complexity — but centralization of Raft and Go-based primitives allows for deterministic ops, easier debug, and stronger safety guarantees. Zero external dependencies once deployed. Competitors:\nOperationally fragmented. DevOps teams must manage multiple platforms (e.g., K8s, Helm, Spark, Airflow). Requires cross-tool observability, secrets management, and governance. ✅ Summary: Why Raft Makes Runink Different Capability Runink (Raft-Powered) Spark / Airflow / K8s Stack State Coordination Raft Consensus Partial (only K8s/etcd) Fault Tolerance HA Replication Tool-dependent Scheduler Raft-backed, deterministic Varies per layer Governance Native, consistent, queryable External Secrets Encrypted + Raft-consistent K8s or env vars Lineage Immutable + auto-tracked External integrations Multitenancy Herds + namespace isolation Namespaces (K8s) Security End-to-end mTLS + RBAC + UIDs Complex setup LLM-native First-class integration Ad hoc orchestration Observability Built-in, unified stack Custom integration Process flow %%{init:{\"fontFamily\":\"monospace\", \"sequence\":{\"showSequenceNumbers\":true}}}%% %% Mermaid Diagram: MapReduce vs RDD vs Raft (Runink) flowchart LR subgraph Normal_Operation[\"✅ Normal Operation (Execution Flow)\"] subgraph MapReduce ID1[\"Input Data 📂\"] MP[\"Map Phase 🛠️\"] SS[\"Shuffle \u0026 Sort Phase 🔀\"] RP[\"Reduce Phase 🛠️\"] ID1 --\u003e MP --\u003e SS --\u003e RP end subgraph RDD RID1[\"Input Data 📂\"] RT[\"RDD Transformations 🔄\"] AT[\"Action Trigger ▶️\"] JS[\"Job Scheduler 📋\"] CE[\"Cluster Execution (Executors) ⚙️\"] OD[\"Output Data 📦\"] RID1 --\u003e RT --\u003e AT --\u003e JS --\u003e CE --\u003e OD end subgraph Raft_Runink RIN[\"Input Data 📂\"] RC[\"Raft Commit (Contracts + Metadata) 🗄️\"] RS[\"Runi Scheduler (Raft-backed) 🧠\"] LW[\"Launch Slices (Isolated Workers) 🚀\"] ROD[\"Output Data 📦\"] RIN --\u003e RC --\u003e RS --\u003e LW --\u003e ROD end end subgraph Failure_Recovery[\"⚡ Failure Recovery Flow (Crash Handling)\"] subgraph MapReduce_Failure MFID[\"Input Data 📂\"] MFR[\"Map Phase Running 🛠️\"] MC[\"Map Node Crash 🛑\"] MF[\"Job Fails Entirely ❌\"] MR[\"Manual Restart Needed 🔄\"] MFID --\u003e MFR --\u003e MC --\u003e MF --\u003e MR end subgraph RDD_Failure RFID[\"Input Data 📂\"] RER[\"RDD Execution Running 🔄\"] RCN[\"RDD Node Crash 🛑\"] DR[\"Driver Attempts Lineage Recompute 🔁\"] PR[\"Partial or Full Job Restart 🔄\"] RFID --\u003e RER --\u003e RCN --\u003e DR --\u003e PR end subgraph Raft_Failure RFD[\"Input Data 📂\"] SR[\"Slice Running 🚀\"] RC[\"Raft Node Crash 🛑\"] EL[\"Raft Detects Loss + Elects New Leader 🧠\"] RE[\"Reschedule Slice Elsewhere ♻️\"] CE[\"Continue Execution Seamlessly ✅\"] RFD --\u003e SR --\u003e RC --\u003e EL --\u003e RE --\u003e CE end end 🚀 How This Model Beats the Status Quo ✅ Compared to Apache Spark Spark (JVM) Runink (Go + Linux primitives) JVM-based, slow cold starts Instantaneous slice spawn using exec Containerized via YARN/Mesos/K8s No container daemon needed Fault tolerance via RDD lineage/logs Strong consistency via Raft Needs external tools for lineage Built-in governance and metadata ✅ Compared to Kubernetes + Airflow Kubernetes / Airflow Runink DAGs stored in SQL, not consistent across API servers DAGs submitted via Raft log, replicated to all Task scheduling needs K8s Scheduler or Celery Runi agents coordinate locally via consensus Containers = overhead Direct exec in a namespaced PID space Secrets are environment or K8s Secret dependent Raft-backed, RBAC-scoped Secrets Manager Governance/logging external Observability and lineage native and real-time 🧠 Conclusion: Go + Linux internals + Raft = Data-Native Compute Runink leverages Raft consensus not just for fault tolerance, but as a foundational architectural choice. It eliminates whole categories of orchestration complexity, state drift, and configuration mismatches by building from first principles — while offering a single runtime that natively understands pipelines, contracts, lineage, and compute.\nIf you’re designing a modern data platform — especially one focused on governance, and efficient domain isolation — Runink is a radically integrated alternative to the Kubernetes-centric model.",
    "description": "1. Architecture \u0026 Paradigm Runink: A Go/Linux-native, vertically integrated data platform that combines execution, scheduling, governance, and observability in a single runtime. Unlike traditional stacks, Runink does not rely on Kubernetes or external orchestrators. Instead, it uses a Raft-based control plane to ensure high availability and consensus across services like scheduling, metadata, and security — forming a distributed operating model purpose-built for data.\nCompetitors: Use a layered, loosely coupled stack:",
    "tags": [],
    "title": "Benchmark",
    "uri": "/runink-site/docs/benchmark/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "🧰 Runink CLI Reference (runi) The runi CLI is the command-line interface to everything in the Runink data platform. It’s your developer-first companion for defining, testing, running, securing, and publishing data pipelines — all from declarative .dsl files and Go-native contracts.\nThis reference describes all available commands, grouped by capability.\n🧱 Project \u0026 Pipeline Lifecycle Command Description runi herd init [project-name] Scaffold a new workspace with starter contracts, features, CI config runi compile --scenario \u003cfile\u003e Generate Go pipeline code from .dsl files runi run --scenario \u003cfile\u003e --contract \u003ccontract.json\u003e Run pipelines locally or remotely runi watch --scenario \u003cfile\u003e Auto-compile \u0026 re-run scenario on save 📁 Schema \u0026 Contract Management Command Description runi contract gen --struct \u003cpkg.Type\u003e Generate a contract from Go struct runi contract diff --old v1.json --new v2.json Show schema drift between versions runi contract rollback Revert to previous contract version runi contract history --name \u003ccontract\u003e Show all versions and changelog entries runi contract validate --file \u003cfile\u003e Validate a file against a contract runi contract catalog Create an index of all contracts in the repo runi contract hash Generate contract hash for versioning 🧪 Testing \u0026 Data Validation Command Description runi synth --dsl feature.dsl --contract feature.contract --golden input.golden Generate synthetic golden test data based on golden files runi audit --dsl feature.dsl --contract feature.contract --golden input.golden Validate pipeline against contract using golden files 🔐 Security, Publishing \u0026 Compliance Command Description runi secure [--sbom|--sign|--scan] Run security audits and generate SBOM runi publish Push metadata, lineage, and contracts to registry runi sbom export [--format spdx] Export SPDX-compliant software bill of materials runi changelog gen Generate changelogs from contract/feature diffs 🔍 Observability \u0026 Lineage Command Description runi lineage --run-id \u003cuuid\u003e Show DAG lineage for a run runi lineage track --source A --sink B Manually link lineage metadata runi lineage graph --out file.dot Export lineage graph in DOT format runi metadata get --key \u003cname\u003e Retrieve stored metadata for a step runi metadata annotate --key \u003cname\u003e Attach annotation to pipeline metadata runi logs --run-id \u003cuuid\u003e View logs for a specific run runi status --run-id \u003cuuid\u003e Check status of a pipeline execution 🤖 Distributed Execution (Remote) Command Description runi deploy --target \u003ck8s|local\u003e Deploy Runi workers to a local or remote cluster runi start --slice \u003cfile\u003e --herd \u003cnamespace\u003e Start execution of a scenario remotely runi kill --run-id \u003cuuid\u003e Terminate running scenario 💪 Control Plane \u0026 Agents Command Description runi herdctl create Create a new Herd (namespace + quotas + policies) runi herdctl delete Delete a Herd runi herdctl update Update Herd quotas, RBAC, metadata runi herdctl list List all Herds and resource states runi herdctl quota set \u003cherd\u003e Update CPU/mem quotas live runi herdctl lineage \u003cherd\u003e View lineage graphs scoped to a Herd runi agentctl list List active Runi agents, resource usage, labels runi agentctl status \u003cagent\u003e Detailed agent status (health, registered slices, metrics) runi agentctl drain \u003cagent\u003e Mark agent as unschedulable (cordon) runi agentctl register Manually register agent (optional bootstrap) runi agentctl cordon \u003cagent\u003e Prevent slice scheduling 🌐 Worker Slice Management Command Description runi slicectl list --herd \u003cid\u003e List all active slices for a Herd runi slicectl logs \u003cslice-id\u003e Fetch logs for a given slice runi slicectl cancel \u003cslice-id\u003e Cancel a running slice gracefully runi slicectl metrics \u003cslice-id\u003e Show real-time metrics for a slice runi slicectl promote \u003cslice-id\u003e Checkpoint a slice mid-run 🔀 Introspection \u0026 Visualization Command Description runi explain --scenario \u003cfile\u003e Describe DAG and step resolution logic runi graphviz --scenario \u003cfile\u003e Render DAG as a .png, .svg, or .dot runi diff --feature old.dsl --feature new.dsl Compare feature files and show logic drift 🧪 REPL \u0026 Exploratory Commands Command Description runi repl Launch interactive DataFrame, SQL, JSON REPL runi json explore -f file.json -q '.email' Run jq-style query on JSON runi query -e \"SELECT * FROM dataset\" Run SQL-like query on scenario input 🛠️ Dev Tools \u0026 Generators Command Description runi gen --dsl input.json Generate feature from sample input runi contract from-feature \u003cfile\u003e Extract contract from .dsl spec runi schema hash Generate contract fingerprint runi bump Auto-increment contract version with changelog 🧹 Plugins \u0026 Extensions Command Description runi plugin install \u003curl\u003e Install external plugin runi plugin list List installed extensions runi plugin run \u003cname\u003e Execute a plugin subcommand 📦 Packaging \u0026 CI/CD Command Description runi build Compile pipeline bundle for remote use runi pack Zip workspace for deployment/distribution runi upgrade Self-update the CLI and plugins runi doctor Diagnose CLI and project setup 📅 Runtime Lifecycle Command Description runi restart --run-id \u003cuuid\u003e Restart a pipeline from last successful checkpoint runi resume --run-id \u003cuuid\u003e Resume paused pipeline without reprocessing runi checkpoint --scenario \u003cfile\u003e Create a persistent step-based checkpoint marker 💬 Collaboration \u0026 Governance Command Description runi comment --contract \u003cfile\u003e Leave inline comments for review (contract-level QA) runi request-approval --contract \u003cfile\u003e Submit contract for governance approval runi feedback --scenario \u003cfile\u003e Attach review notes to a scenario 🛡️ Privacy, Redaction \u0026 Data Escrow Command Description runi redact --contract \u003cfile\u003e Automatically redact PII based on tags runi escrow --run-id \u003cuuid\u003e Encrypt pipeline outputs for future unsealing runi anonymize --input \u003cfile\u003e Generate synthetic version of a sensitive input file 🗓 Event-Based Execution Command Description runi trigger --on \u003cwebhook|s3|pubsub\u003e Set up trigger-based pipeline starts runi listen --event \u003ctype\u003e Listen for external event to start scenario runi subscribe --stream \u003csource\u003e Subscribe to stream source with offset recovery 🔄 Pipeline \u0026 Contract Lifecycle Command Description runi freeze --scenario \u003cfile\u003e Lock DAG hash and contract state as immutable runi archive --herd \u003cname\u003e --keep \u003cN\u003e Archive old scenarios/runs beyond retention policy runi retire --contract \u003cfile\u003e Deprecate contract from active use 🧬 Metadata Graph \u0026 Semantic Search Command Description runi knowledge export --format turtle Export contract and DAG metadata as RDF runi query lineage Run SQL-style queries across lineage metadata 💬 Use runi \u003ccommand\u003e --help for flags, options, and examples.\nRunink’s CLI gives you a full stack data engine in your terminal — from contracts to clusters, from .dsl to full observability.",
    "description": "🧰 Runink CLI Reference (runi) The runi CLI is the command-line interface to everything in the Runink data platform. It’s your developer-first companion for defining, testing, running, securing, and publishing data pipelines — all from declarative .dsl files and Go-native contracts.\nThis reference describes all available commands, grouped by capability.\n🧱 Project \u0026 Pipeline Lifecycle Command Description runi herd init [project-name] Scaffold a new workspace with starter contracts, features, CI config runi compile --scenario \u003cfile\u003e Generate Go pipeline code from .",
    "tags": [],
    "title": "CLI Reference",
    "uri": "/runink-site/docs/cli/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "🆘 Runink CLI: Help Template This is a developer-friendly help template for implementing consistent runi \u003ccommand\u003e --help outputs.\n🧱 Format: Basic Help Command runi \u003ccommand\u003e [subcommand] [flags] Usage: runi \u003ccommand\u003e [options] Options: -h, --help Show this help message and exit -v, --verbose Show detailed logs and diagnostics Example: runi init --help Initialize a new Runink project. Usage: runi init [project-name] Flags: -h, --help Show help for init 🔄 Example: runi compile --help runi compile --scenario \u003cfile.dsl\u003e Description: Compile a `.dsl` scenario and its contract into an executable Go DAG. Generates a Go file under `rendered/` based on contract-linked step tags. Usage: runi compile --scenario features/trade_cdm.dsl Flags: --scenario Path to a DSL scenario file --out Optional: custom output path for DAG (default: rendered/\u003cname\u003e.go) --dry-run Only validate scenario and contract, do not write DAG --verbose Show full DAG step resolution logs 🧪 Example: runi test --help runi test --scenario \u003cfile.dsl\u003e Description: Execute a feature scenario with golden test inputs and compare output. Supports diff mode and golden update flows. Usage: runi test --scenario features/onboard.dsl Flags: --scenario DSL file to test --golden Optional: override path to golden test folder --update Automatically update golden output on success --only \u003cstep\u003e Run test up to a specific pipeline step 🔐 Example: runi contract gen --help runi contract gen --struct \u003cpackage.Type\u003e --out \u003cfile\u003e Description: Generate a JSON contract definition from a Go struct. Includes schema, access tags, and validation metadata. Usage: runi contract gen --struct contracts.Customer --out contracts/customer.json Flags: --struct Fully qualified Go type (e.g. contracts.Customer) --out Output contract file path --flatten Inline nested types into flat fields --herd Optional: attach to specific herd (e.g. finance) runi contract diff --help Diff two versions of a contract and show schema drift. Usage: runi contract diff --old v1.json --new v2.json runi run --help Run a compiled pipeline with data inputs. Usage: runi run --scenario \u003cfile.dsl\u003e [--contract file] [--herd name] Flags: --scenario Scenario to execute --contract Optional explicit contract --herd Herd to run pipeline in --dry-run Preview DAG resolution only runi lineage --help Show lineage metadata for a run. Usage: runi lineage --run-id \u003cid\u003e Flags: --run-id Unique run identifier --output Format (json|csv|graph) runi publish --help Publish contracts, lineage, and tags to metadata registry. Usage: runi publish --herd \u003cname\u003e [--scenario file] runi repl --help Start interactive REPL for querying test inputs or contract data. Usage: runi repl --scenario \u003cpath\u003e 🤖 Example: runi deploy --help runi deploy --target \u003ctarget\u003e Description: Deploy Runi workers and slices to a remote orchestration cluster. Usage: runi deploy --target k8s Flags: --target Target platform (k8s, bigmachine) --herd Herd (namespace) to deploy into --dry-run Simulate deployment without applying --confirm Require manual confirmation for remote changes runi schedule --help Schedule a pipeline scenario for recurring execution. Usage: runi schedule --scenario \u003cfile\u003e --cron \"0 6 * * *\" Flags: --scenario DSL file --cron Cron-style expression runi audit --help Show schema contract change history and approvals. Usage: runi audit --contract \u003cfile\u003e runi restart --help Restart a failed or incomplete pipeline run from its last checkpoint. Usage: runi restart --run-id \u003cuuid\u003e Flags: --run-id Run ID to restart from --force Ignore checkpoint and rerun from start runi resume --help Resume an interrupted or paused pipeline. Usage: runi resume --run-id \u003cuuid\u003e runi checkpoint --help Create a DAG state checkpoint for partial run recovery. Usage: runi checkpoint --scenario \u003cfile\u003e runi comment --help Leave inline comments for contracts or fields (used in review tools). Usage: runi comment --contract \u003cfile\u003e --field \u003cpath\u003e --note \u003ctext\u003e runi request-approval --help Submit a contract for governance approval and audit. Usage: runi request-approval --contract \u003cfile\u003e runi feedback --help Attach feedback note to a scenario feature for peer review. Usage: runi feedback --scenario \u003cfile\u003e --note \u003ctext\u003e runi redact --help Automatically redact fields marked pii:\"true\" in a contract schema. Usage: runi redact --contract \u003cfile\u003e --out \u003cfile\u003e runi escrow --help Encrypt and store output data for delayed release or approval. Usage: runi escrow --run-id \u003cuuid\u003e --out \u003cvault.json\u003e runi anonymize --help Create a non-sensitive version of input using faker + tags. Usage: runi anonymize --input \u003cfile\u003e --contract \u003cfile\u003e --out \u003cfile\u003e runi trigger --help Define an event trigger for this scenario. Usage: runi trigger --scenario \u003cfile\u003e --on webhook|s3|pubsub runi listen --help Start a listener to monitor incoming event and dispatch pipeline. Usage: runi listen --event \u003ctype\u003e runi subscribe --help Subscribe to a streaming topic or channel with offset tracking. Usage: runi subscribe --stream \u003ctopic\u003e --window 5m runi freeze --help Freeze contract + scenario versions with hashes for snapshot validation. Usage: runi freeze --scenario \u003cfile\u003e runi archive --help Archive old versions of scenarios and their runs by herd. Usage: runi archive --herd \u003cname\u003e --keep 3 runi retire --help Retire a contract so it cannot be used in future scenarios. Usage: runi retire --contract \u003cfile\u003e runi lineage graph --help Export full DAG and contract lineage as GraphViz dot file. Usage: runi lineage graph --out lineage.dot runi knowledge export --help Export pipeline metadata using RDF serialization (Turtle/N-Triples). Usage: runi knowledge export --format turtle runi query lineage --help Query lineage metadata using SQL-like syntax. Usage: runi query lineage --sql \"SELECT * WHERE pii = true\" runi openai audit --help Use an LLM to summarize contract diffs or suggest field comments. Usage: runi openai audit --contract \u003cfile\u003e runi sandbox --help Execute scenario in a secure ephemeral environment. Usage: runi sandbox --scenario \u003cfile\u003e runi simulate --help Replay input data as a stream window to test stateful logic. Usage: runi simulate --input \u003cfile\u003e --window 5m runi mint-token --help Generate a short-lived JWT for scoped access by herd or scenario. Usage: runi mint-token --herd finance --role analyst --ttl 5m 🧠 Best Practices ✅ Describe what the command does, not how it’s implemented ✅ Include at least 1 usage example ✅ Use consistent flags: --scenario, --contract, --out, --herd ✅ Provide guidance for --dry-run, --verbose, --help ✅ Include multi-step examples if command touches multiple files",
    "description": "🆘 Runink CLI: Help Template This is a developer-friendly help template for implementing consistent runi \u003ccommand\u003e --help outputs.\n🧱 Format: Basic Help Command runi \u003ccommand\u003e [subcommand] [flags] Usage: runi \u003ccommand\u003e [options] Options: -h, --help Show this help message and exit -v, --verbose Show detailed logs and diagnostics Example: runi init --help Initialize a new Runink project. Usage: runi init [project-name] Flags: -h, --help Show help for init 🔄 Example: runi compile --help runi compile --scenario \u003cfile.",
    "tags": [],
    "title": "CLI Help Commands",
    "uri": "/runink-site/docs/clihelp/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "🤝 Contributing to Runink Welcome! First off, thank you for considering contributing to Runink. We deeply appreciate your support and effort to improve our project.\nThis document will guide you through the process of contributing code, filing issues, suggesting features, and participating in the Runink community.\n📜 Code of Conduct We expect everyone participating to adhere to our Code of Conduct (to be created). Respect and kindness are the foundation.\n🛠️ How to Contribute 1. Fork the Repo Use GitHub’s “Fork” button to create a personal copy of the repository.\n2. Clone Your Fork git clone https://github.com/your-username/runink.git cd runink 3. Create a New Branch Use a clear branch naming convention:\ngit checkout -b feature/short-description # or git checkout -b fix/bug-description 4. Make Your Changes Follow our coding guidelines:\nWrite idiomatic Go (gofmt, golint). Keep PRs small and focused. Update or add tests for your changes. Update documentation (docs/) if applicable. 5. Test Before You Push Run all tests:\nmake lint make test 6. Push and Open a Pull Request Push to your fork and open a Pull Request against the main branch.\ngit push origin feature/short-description On GitHub, create a new Pull Request and fill in the template (title, description, related issues).\n📋 Development Guidelines CLI Commands: Place new commands inside their respective domain folder (barnctl, buildctl, herdctl, runictl). Testing: Add unit tests for CLI commands, helpers, validators. Logging: Use structured logging where needed. Security: Always consider security (no plaintext secrets, minimal privilege). Performance: Avoid premature optimization, but don’t introduce obvious inefficiencies. 🔍 Reporting Bugs Search existing issues first. File a new issue with clear reproduction steps. Provide logs, stack traces, and your environment (OS, Go version). If you discover a security vulnerability, please do not open a public issue.\nInstead, email us at paes@dashie.ink.\n🚀 Suggesting Features Open an Issue labeled enhancement. Explain your use case and how it aligns with Runink’s vision. ❤️ Code of Conduct We’re a community of data builders. We expect contributors to be respectful, inclusive, and constructive.\nPlease read our Code of Conduct before contributing.\n🧵 Join the Community GitHub Discussions (coming soon) Discord server (invite coming soon) Follow our roadmap in docs/roadmap.md 📅 Regular Updates We sync main with active development regularly. Expect fast iteration and reviews.\nIf you have any questions, feel free to open an issue or discussion!\nThanks for being part of the Runink Herd and for helping us build the future of safe, expressive, and reliable data pipelines.🐑\nWe can’t wait to see what you contribute! 🙌\n— The Runink Team",
    "description": "🤝 Contributing to Runink Welcome! First off, thank you for considering contributing to Runink. We deeply appreciate your support and effort to improve our project.\nThis document will guide you through the process of contributing code, filing issues, suggesting features, and participating in the Runink community.\n📜 Code of Conduct We expect everyone participating to adhere to our Code of Conduct (to be created). Respect and kindness are the foundation.",
    "tags": [],
    "title": "Contributing",
    "uri": "/runink-site/docs/contributing/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Data Lineage \u0026 Metadata Tracking – Runink Runink pipelines are designed to be fully traceable, auditable, and schema-aware. With built-in lineage support, every pipeline can generate:\nVisual DAGs of data flow and dependencies Metadata snapshots with schema versions and field hashes Run-level logs for audit, debugging, and compliance This guide walks through how Runink enables robust data observability and governance by default.\n🔍 What Is Data Lineage? Lineage describes where your data came from, what happened to it, and where it went.\nIn Runink, every pipeline run captures:\nSources: file paths, streaming URIs, tags Stages: steps applied, transform versions Contracts: schema file, struct, and hash Sinks: output paths, filters, conditions Run metadata: timestamps, roles, record count 📈 Generate a Lineage Graph runi lineage --scenario features/orders.dsl --out lineage/orders.svg The graph shows:\nInputs and outputs All applied steps Contract versions and field diff hashes Optional labels (e.g., role, source, drift) 🧾 Per-Run Metadata Log Every run emits a record like:\n{ \"run_id\": \"run-20240423-abc123\", \"stage\": \"JoinUsersAndOrders\", \"contract\": \"user_order_v2.json\", \"schema_hash\": \"b72cd1a\", \"records_processed\": 9123, \"timestamp\": \"2024-04-23T11:02:00Z\", \"role\": \"analytics\", \"drift_detected\": false } 🧪 Snapshotting \u0026 Version Tracking You can snapshot inputs/outputs with:\nruni snapshot --contract contracts/user.json --out snapshots/users_2024-04-23.json And later compare against historical output.\n🚨 Drift Detection Runink detects when incoming data deviates from expected contract:\nruni contract diff --old v1.json --new incoming.json Or as part of a scenario run:\nruni run --verify-contract This flags:\nMissing/extra fields Type mismatches Tag mismatches (e.g., missing pii, access) 🔐 Metadata for Compliance Attach metadata to every stage:\ntype StageMetadata struct { RunID string Role string Contract string Hash string Source string Timestamp string } Send this to a:\nDocument DB (e.g. Mongo) Data lake (e.g. MinIO, S3) Audit stream (e.g. Kafka topic) 📡 Monitoring \u0026 Observability Runink supports Prometheus metrics per stage:\nruni_records_processed_total runi_stage_duration_seconds runi_schema_drift_detected_total runi_invalid_records_total 🧠 Example Use Cases Role How Lineage Helps Data Engineer Debug broken joins, drift, formats Analyst Understand where numbers came from Governance Prove schema conformance ML Engineer Snapshot training input lineage Summary Runink provides end-to-end data lineage as a first-class feature, not an afterthought:\nBuilt-in visual DAGs Contract + transform metadata Auditable, role-aware stage outputs Real-time observability with metrics Lineage lets you move fast without breaking trust.",
    "description": "Data Lineage \u0026 Metadata Tracking – Runink Runink pipelines are designed to be fully traceable, auditable, and schema-aware. With built-in lineage support, every pipeline can generate:\nVisual DAGs of data flow and dependencies Metadata snapshots with schema versions and field hashes Run-level logs for audit, debugging, and compliance This guide walks through how Runink enables robust data observability and governance by default.\n🔍 What Is Data Lineage? Lineage describes where your data came from, what happened to it, and where it went.",
    "tags": [],
    "title": "Data Lineage",
    "uri": "/runink-site/docs/data-lineage/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "📘 Feature DSL — Authoring Pipelines in Natural Language Runink’s .dsl files allow data, governance, and domain teams to write declarative pipeline logic in plain English — no YAML, no code, just structured steps tied to contracts.\nInspired by Gherkin and feature-driven development, the DSL is intentionally designed to:\nAlign with real-world data contracts Support lineage, compliance, and multi-tenant governance Be editable by non-engineers — analysts, stewards, and reviewers ✨ Full Example Feature: Trade Events – Validation \u0026 Compliance Scenario: Validate and Tag Incoming Financial Trade Events Metadata: purpose: \"Check and tag incoming trade events for compliance and data quality\" module_layer: \"Silver\" herd: \"Finance\" slo: \"99.9%\" classification: \"pii\" contract: \"cdm_trade/fdc3events.contract\" contract_version: \"1.0.0\" compliance: [\"SOX\", \"GDPR\", \"PCI-DSS\"] lineage_tracking: true Given: \"Arrival of Events\" - source_type: stream - name: \"Trade Events Kafka Stream\" - format: CDM - tags: [\"live\", \"trades\", \"finance\"] When \"Data is received\": - Decode each trade event using the CDM schema - Check for required fields: trade_id, symbol, price, timestamp - Mask sensitive values like SSNs, emails, and bank accounts - Tag events with classification and region - Compare schema to the latest approved contract version Then: - Send valid records to: snowflake table \"Validated Trades Table\" - Send invalid records to: snowflake table \"DLQ for Invalid Trades\" - Annotate all records with compliance and lineage metadata Assertions: - At least 1,000 records must be processed - Schema drift must not be detected - All sensitive fields must pass redaction or tokenization checks GoldenTest: input: \"cdm_trade/fdc3events.input\" output: \"cdm_trade/data/fdc3events.validated.golden\" validation: strict Notifications: - On schema failure → alert \"alerts/finance_data_validation\" - On masking failure → alert \"alerts/finance_security_breach\" 🧠 DSL Concepts Block Description Feature High-level business intent (group of scenarios) Scenario Specific pipeline run, often tied to a contract version Metadata Tags used for governance, lineage, compliance, and SLOs Given Declares the data source and input assumptions When Describes logic, transformations, and validations to apply Then Declares output actions — writing to sinks, tagging, alerts Assertions Validate record counts, masking, schema drift, etc. GoldenTest Points to expected inputs/outputs for regression safety Notifications Alerts emitted when failures occur during pipeline runs 🔍 Metadata-Driven Pipelines Each .dsl is contract-aware and herd-scoped by default.\nContracts are declared via contract: and contract_version: SLOs, classification, and compliance tags are baked into Metadata Data lineage and observability are auto-inferred from DSL structure ✅ DSL Advantages Self-documenting: Reads like an audit policy + data spec Secure-by-default: Every scenario runs inside a herd Governance-friendly: Tracks lineage, policy, SLOs, classification Reproducible: GoldenTest ensures outputs match expectations Declarative: No code, no imperative orchestration logic 📎 Tips for Authors Use this Instead of - Mask sensitive values... @step(\"FieldMasker\") \"Validate and Tag...\" \"run pipeline X\" Plain-English assertions Inline test logic contract: \"x.contract\" Hardcoded field lists 📚 Related Links 📑 Schema Contracts 🧬 Data Lineage 🧪 Testing Pipelines 🎯 Final Thought With Runink DSL, your data pipeline is the spec — no translation layers, no wasted doc effort. Write what the pipeline should do, tag it with intent, and Runink turns it into secure, auditable, executable logic.\nLet your domain experts lead the way — and let infra follow automatically.",
    "description": "📘 Feature DSL — Authoring Pipelines in Natural Language Runink’s .dsl files allow data, governance, and domain teams to write declarative pipeline logic in plain English — no YAML, no code, just structured steps tied to contracts.\nInspired by Gherkin and feature-driven development, the DSL is intentionally designed to:\nAlign with real-world data contracts Support lineage, compliance, and multi-tenant governance Be editable by non-engineers — analysts, stewards, and reviewers ✨ Full Example Feature: Trade Events – Validation \u0026 Compliance Scenario: Validate and Tag Incoming Financial Trade Events Metadata: purpose: \"Check and tag incoming trade events for compliance and data quality\" module_layer: \"Silver\" herd: \"Finance\" slo: \"99.",
    "tags": [],
    "title": "Feature DSL",
    "uri": "/runink-site/docs/feature-dsl/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Getting Started with Runink Welcome to Runink! This quick-start guide will help you get up and running with Runink to effortlessly build, test, and run data pipelines.\n🚀 1. Installation Make sure you have Go installed (v1.20 or later). Then install Runink:\ngo install github.com/runink/runink@latest Ensure $GOPATH/bin is in your $PATH.\n🛠 2. Initialize Your Project Create a new Runink project in seconds:\nruni init myproject cd myproject This command generates:\nInitial Go module Sample contracts Example .dsl files Golden file tests Dockerfile and CI/CD configs 📋 3. Explore the Project Structure Your project includes:\nmyproject/ ├── bin/ -\u003e CLI ├── contracts/ -\u003e Schema contracts and transformation logic on Go structs ├── features/ -\u003e Scenarios definitions for each feature from the `.dsl` files ├── golden/ -\u003e Golden files used in regression testing with examples and synthetic data ├── dags/ -\u003e Generated DAG code from the contracts and features to be executed by Runi ├── herd/ -\u003e Domain Service Control Policies (Herd context isolation) ├── barn/ -\u003e Runi Agent manager: cgroups, metrics, logs, gRPC control plane ├── docs/ -\u003e Markdown docs, examples, use cases, and playbooks └── .github/workflows/ -\u003e CI/CD and test pipelines ⚙️ 4. Compile and Run Pipelines Compile your first pipeline:\nruni compile --scenario features/example.dsl --out pipeline/example.go --herd my-data-herd Execute a scenario:\nruni run --scenario features/example.dsl --herd my-data-herd ✅ 5. Test Your Pipelines Use built-in golden testing to ensure correctness:\nruni audit --scenario features/example.dsl --herd my-data-herd runi synth --scenario features/example.dsl --herd my-data-herd If logic changes are intentional, update golden files:\nruni update --scenario features/example.dsl --update --herd my-data-herd 🔍 6. Interactive REPL Explore and debug data interactively:\nruni fetch --scenario features/example.dsl --herd my-data-herd Example REPL commands:\nload csv://data/input.csv apply MyTransform show 📚 7. Next Steps 📖 Learn the Feature DSL Syntax to write expressive data pipelines 🔎 Explore Data Lineage \u0026 Metadata for auditability and reproducibility 📦 Understand Schema \u0026 Contract Management to ensure schema validation and drift detection 🚧 Support \u0026 Community Need help or have suggestions?\nOpen an issue on GitHub Join our community discussions and get involved! Let’s build secure, fast, and auditable pipelines — the Go-native way, with Runink.",
    "description": "Getting Started with Runink Welcome to Runink! This quick-start guide will help you get up and running with Runink to effortlessly build, test, and run data pipelines.\n🚀 1. Installation Make sure you have Go installed (v1.20 or later). Then install Runink:\ngo install github.com/runink/runink@latest Ensure $GOPATH/bin is in your $PATH.\n🛠 2. Initialize Your Project Create a new Runink project in seconds:\nruni init myproject cd myproject This command generates:",
    "tags": [],
    "title": "Getting Started",
    "uri": "/runink-site/docs/getting_started/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "🧭 Code of Conduct Welcome to the Runink community! We’re building a secure, auditable, and open orchestration platform — and we’re doing it together.\nWe are committed to providing a welcoming and inclusive environment for everyone, regardless of background, experience level, identity, or role.\n📜 Our Pledge In the interest of fostering an open and inclusive community, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone.\nWe commit to:\nUsing welcoming and inclusive language Being respectful of differing viewpoints and experiences Accepting constructive feedback gracefully Demonstrating empathy toward others Prioritizing collaboration over correctness 🚫 Unacceptable Behavior Examples of unacceptable behavior include:\nInsults, derogatory comments, or personal attacks Trolling, baiting, or deliberate disruption of conversation Unwelcome sexual attention or advances Publishing private information without consent Discriminatory jokes or language (even if “just a joke”) Dismissing someone’s work or identity based on background 📣 Reporting If you experience or witness unacceptable behavior, please report it by contacting:\n📧 conduct@runink.dev\nAll reports will be handled confidentially. We are committed to protecting your privacy and ensuring fair review.\n⚖️ Enforcement Project maintainers are responsible for upholding this Code of Conduct. They will take appropriate and fair corrective action in response to any behavior they deem inappropriate, threatening, offensive, or harmful.\nMaintainers have the right to remove comments, commits, issues, pull requests, or contributors that violate this code — temporarily or permanently.\n🤝 Scope This Code of Conduct applies to all:\nGitHub repositories, issues, pull requests, and discussions Community forums and Discord/Slack channels Conferences, meetups, and virtual events Official documentation and external communications 📘 Attribution This Code of Conduct is adapted from the Contributor Covenant, version 2.1.\nThank you for helping us create a respectful, professional, and inclusive environment. Let’s build Runink together — openly and responsibly.",
    "description": "🧭 Code of Conduct Welcome to the Runink community! We’re building a secure, auditable, and open orchestration platform — and we’re doing it together.\nWe are committed to providing a welcoming and inclusive environment for everyone, regardless of background, experience level, identity, or role.\n📜 Our Pledge In the interest of fostering an open and inclusive community, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone.",
    "tags": [],
    "title": "Code of Conduct",
    "uri": "/runink-site/docs/code_of_conduct/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Runink Glossary This glossary defines key terms, acronyms, and concepts used throughout the Runink documentation and codebase.\n.dsl File A human-readable file written in Gherkin syntax used to describe a data pipeline scenario using Given/When/Then structure and tags like @source, @step, and @sink.\nBDD (Behavior-Driven Development) A software development approach that describes application behavior in plain language, often used with .dsl files.\nGolden File A snapshot of the expected output from a pipeline or transformation, used to assert correctness in automated tests.\nSchema Contract A versioned definition of a data structure (e.g., JSON, Protobuf, Go struct) used to validate pipeline input/output and detect schema drift.\nSchema Drift An unintended or unexpected change in a schema that may break pipeline compatibility.\nDAG (Directed Acyclic Graph) A graph of pipeline stages where each edge represents a dependency, and there are no cycles. Used for orchestrating non-linear workflows.\nDLQ (Dead Letter Queue) A place to store invalid or failed records so they can be analyzed and retried later without interrupting the rest of the pipeline.\nREPL (Read-Eval-Print Loop) An interactive interface that lets users type commands and immediately see the results. Runink’s REPL supports loading data, applying steps, and viewing outputs.\nLineage A traceable path showing how data flows through each pipeline stage, from source to sink, including what transformed it and which contract was applied.\nPrometheus A monitoring system used to collect and store metrics from pipeline executions.\nOpenTelemetry An observability framework for collecting traces and metrics, helping to visualize the execution path and performance of pipelines.\ngRPC A high-performance, open-source universal RPC framework used for running distributed pipeline stages.\nProtobuf (Protocol Buffers) A method for serializing structured data, used in gRPC communication and schema definitions.\nBigmachine A Go library for orchestrating distributed, stateless workers. Used by Runink to scale pipelines across multiple machines.\nContract Hash A hash value generated from a schema contract to uniquely identify its version. Used for detecting changes and tracking usage.\nSBOM (Software Bill of Materials) A manifest of all dependencies and components included in a software release, used for compliance and security auditing.\nFDC3 (Financial Desktop Connectivity and Collaboration Consortium) A standard for interop between financial applications. Runink can integrate with FDC3 schemas and messaging models.\nCDM (Common Domain Model) A standardized schema used in finance and trading to represent products, trades, and events. Supported natively by Runink.\nHave a term you’d like added? Open an issue or suggest a change in the docs!",
    "description": "Runink Glossary This glossary defines key terms, acronyms, and concepts used throughout the Runink documentation and codebase.\n.dsl File A human-readable file written in Gherkin syntax used to describe a data pipeline scenario using Given/When/Then structure and tags like @source, @step, and @sink.\nBDD (Behavior-Driven Development) A software development approach that describes application behavior in plain language, often used with .dsl files.\nGolden File A snapshot of the expected output from a pipeline or transformation, used to assert correctness in automated tests.",
    "tags": [],
    "title": "Glossary",
    "uri": "/runink-site/docs/glossary/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "🗺️ Runink Roadmap Welcome to the official Runink Roadmap — our evolving guide to what we’re building, where we’re headed, and how you can get involved.\nRunink is built on the belief that modern data platforms should be safe by default, composable by design, and collaborative at scale. This roadmap reflects our commitment to transparency, community-driven development, and rapid iteration.\n🧩 Roadmap Themes Theme Description Composable Pipelines Make it easy to build, reuse, and test pipeline steps across teams and domains. Secure \u0026 Compliant by Default Tighten RBAC, data contracts, and observability for enterprise-grade governance. DevX \u0026 Developer Productivity Empower devs with a powerful CLI, REPL, codegen, and rapid iteration loops. Streaming-First DataOps Advance real-time use cases with backpressure-safe, contract-aware streaming. Interoperability \u0026 Ecosystem Play well with FDC3, CDM, OpenLineage, Kafka, Snowflake, and more. 🧭 Current Focus (Q2 2025) These items are in active development or early testing:\nHerd Namespace Isolation (multi-tenant namespace support) Golden Test Rewrites for easier review and diffing CLI REPL SQL Mode with DataFrame-to-Feature export RBAC \u0026 Token Scoping Enhancements Raft-backed Barn \u0026 Secrets Manager Integration gRPC Streaming Orchestration Pipeline Preview Mode (dry-run with metadata only) Lineage UI + CLI support Remote Artifact Signing \u0026 SBOM generation (SLSA-style) 🔜 Near-Term (Q3 2025) Planned next based on user feedback and enterprise needs:\nLive Feature File Linter \u0026 Formatter REPL Session Recorder (record → replay feature building) Multi-Herd Scheduling \u0026 Cost Reporting Secrets Rotation + External Vault Integration Contract Diff Web Viewer Push-to-Registry UX from CLI DLQ Visualization + Retry Tools Plugin Marketplace (source/sink/step handlers) 🌅 Long-Term Vision (Late 2025+) Our long-range goals to shape Runink into the standard platform for responsible data pipelines:\n⚙️ Full No-YAML Orchestration (Declarative-Only Pipelines) 🧠 AI Copilot for Contract \u0026 Scenario Generation 🌐 Cross-Org Data Mesh Support via Herd Federation 📡 Runink Cloud (fully managed, secure, multi-tenant SaaS) 🔒 Zero-Trust Data Contracts (ZK + Provenance) 🧠 Ideas We’re Exploring These are in research/design phases — feedback welcome!\n✨ Feature DSL Step Suggestions in CLI 🔀 Schema Merge Conflict Resolution UX 📥 Native ingestion support for S3/Parquet/Arrow 🔎 Full integration with OpenLineage + dbt Core 🧾 GitHub Copilot integration for contract authoring 🙋‍♀️ Contribute to the Roadmap We prioritize what the community and users need most. If there’s a feature you’d love to see:\nOpen an issue using the Feature Request template Upvote existing roadmap items via 👍 reactions Join upcoming roadmap discussions (Discord coming soon!) PRs welcome for anything marked as help-wanted 🔄 Release Cadence We aim for:\nMinor releases every 4–6 weeks (feature drops, improvements) Patch releases as needed (hotfixes, regressions) Major milestones every ~6 months with community showcases Track progress in CHANGELOG.md\nThanks for being part of the journey — we’re building Runink with you, not just for you. Let’s define the future of safe, modular, and explainable data platforms together.\n— Team Runink 🐑",
    "description": "🗺️ Runink Roadmap Welcome to the official Runink Roadmap — our evolving guide to what we’re building, where we’re headed, and how you can get involved.\nRunink is built on the belief that modern data platforms should be safe by default, composable by design, and collaborative at scale. This roadmap reflects our commitment to transparency, community-driven development, and rapid iteration.\n🧩 Roadmap Themes Theme Description Composable Pipelines Make it easy to build, reuse, and test pipeline steps across teams and domains.",
    "tags": [],
    "title": "Roadmap",
    "uri": "/runink-site/docs/roadmap/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "🚀 Runink Quickstart: CDM Trade Pipeline This example shows how to define, test, apply, and run a declarative data pipeline using Runink.\nEnvironment scenario %% Mermaid Diagram: Runink Architecture (Blueprint View) flowchart TD subgraph Developer_Client[\"🌐 Developer / Client\"] Developer[\"Developer\"] end subgraph Global_Control_Plane[\"🧭 Runink Global Control Plane (HA Setup)\"] GlobalAPI[\"API Server x3 - AuthN/AuthZ - Herd Routing - TLS gRPC\"] HerdDirectory[\"Herd Directory - Maps Herds to Raft Groups - Metadata Routing\"] end subgraph Finance_Herd[\"🏦 Finance Herd Partition\"] FinanceScheduler[\"Finance Scheduler (Leader) - DAG Planning - Placement Decisions\"] FinanceBarn[\"Finance Barn (KV Store) - BadgerDB (Local)\"] FinanceGovernance[\"Finance Governance Service - Lineage - Quality - Contracts\"] FinanceSecrets[\"Finance Secrets Manager - Raft-backed Secret Storage\"] FinanceRaft[\"Finance Raft Group (5 Nodes) (etcd-io/raft)\"] end subgraph Analytics_Herd[\"📊 Analytics Herd Partition\"] AnalyticsScheduler[\"Analytics Scheduler (Leader) - DAG Planning - Placement Decisions\"] AnalyticsBarn[\"Analytics Barn (KV Store) - BadgerDB (Local)\"] AnalyticsGovernance[\"Analytics Governance Service - Lineage - Quality - Contracts\"] AnalyticsSecrets[\"Analytics Secrets Manager - Raft-backed Secret Storage\"] AnalyticsRaft[\"Analytics Raft Group (5 Nodes) (etcd-io/raft)\"] end subgraph Worker_Cluster[\"🧱 Worker Nodes Cluster\"] RuniAgent[\"Runi Agent x100 - Node Registration - Slice Management - Metrics Collection\"] RuniSlice[\"Runi Slice (Ephemeral Container) - Herd Namespaced - Config Loaded - Secrets Injected\"] end Developer --\u003e | CLI/API Requests | GlobalAPI GlobalAPI --\u003e | Resolve Herd Assignment | HerdDirectory GlobalAPI --\u003e | Finance Pipelines | FinanceScheduler GlobalAPI --\u003e | Analytics Pipelines | AnalyticsScheduler FinanceScheduler --\u003e | DAG and Placement Reads | FinanceBarn FinanceGovernance --\u003e | Metadata/Lineage Writes | FinanceBarn FinanceSecrets --\u003e | Secrets CRUD | FinanceBarn FinanceBarn --\u003e | Log Replication | FinanceRaft AnalyticsScheduler --\u003e | DAG and Placement Reads | AnalyticsBarn AnalyticsGovernance --\u003e | Metadata/Lineage Writes | AnalyticsBarn AnalyticsSecrets --\u003e | Secrets CRUD | AnalyticsBarn AnalyticsBarn --\u003e | Log Replication | AnalyticsRaft FinanceScheduler --\u003e | Dispatch Finance Slices | RuniAgent AnalyticsScheduler --\u003e | Dispatch Analytics Slices | RuniAgent RuniAgent --\u003e | Launch with Herd Isolation | RuniSlice RuniAgent --\u003e | Fetch Finance Secrets | FinanceSecrets RuniAgent --\u003e | Fetch Analytics Secrets | AnalyticsSecrets RuniSlice --\u003e | Emit Lineage Events | FinanceGovernance RuniSlice --\u003e | Emit Lineage Events | AnalyticsGovernance RuniSlice --\u003e | Expose Service Port | RuniAgent RuniAgent --\u003e | Port-Forwarded Access | Developer 🛠️ Prerequisites Ensure you have:\nA .dsl scenario: features/cdm_trade/trade_cdm.dsl A Go contract file: contracts/trade_cdm_multi.go Golden test files: golden/cdm_trade/ Sample input data: golden/cdm_trade/input.json Our example presents the following:\nflowchart TD A[\"Kafka (raw)\"] --\u003e B[\"DecodeCDMEvents\"] B --\u003e C[\"sf:control.decoded_cdm_events\"] B --\u003e D[\"ValidateLifecycle\"] D --\u003e|if !valid| E[\"sf:control.invalid_cdm_events\"] D --\u003e|if valid| F[\"TagWithFDC3Context\"] F --\u003e G[\"sf:cdm.validated_trade_events\"] 💡 Example Flow # Create a secure namespace (herd) runi herd init finance runi compile --scenario features/payment.dsl --herd finance --contract contracts/payment.go --out dags/payment.go runi run --dag dags/payment.go 🧪 Test Your Pipelines runi audit --scenario features/payment.dsl --contract contracts/payment.go --golden tests/input.json runi synth --scenario features/payment.dsl --contract contracts/payment.go --golden tests/input.json runi fetch --scenario features/example.dsl --golden tests/input.json --output table.sql --show 📊 Inspect Pipeline Execution After running, inspect the pipeline using:\nruni status --run-id RUN-20240424-XYZ --herd finance Example Output run_id: RUN-20240424-XYZ herd: finance status: completed steps: - DecodeCDMEvents: processed: 2 output: sf://control.decoded_cdm_events - ValidateLifecycle: passed: 1 failed: 1 output: - valid → sf://cdm.validated_trade_events - invalid → sf://control.invalid_cdm_events - TagWithFDC3Context: enriched: 1 context_prefix: fdc3.instrumentView: lineage: contract_hash: a9cd23f… contract_version: v3 created_by: service-account:etl-runner 🔍 Follow-Up Commands runi lineage --run-id RUN-20240424-XYZ runi logs --run-id RUN-20240424-XYZ runi publish --herd finance --scenario features/cdm_trade/cdm_trade.dsl Runink makes secure, declarative data orchestration easy — every pipeline is testable, auditable, and reproducible.",
    "description": "🚀 Runink Quickstart: CDM Trade Pipeline This example shows how to define, test, apply, and run a declarative data pipeline using Runink.\nEnvironment scenario %% Mermaid Diagram: Runink Architecture (Blueprint View) flowchart TD subgraph Developer_Client[\"🌐 Developer / Client\"] Developer[\"Developer\"] end subgraph Global_Control_Plane[\"🧭 Runink Global Control Plane (HA Setup)\"] GlobalAPI[\"API Server x3 - AuthN/AuthZ - Herd Routing - TLS gRPC\"] HerdDirectory[\"Herd Directory - Maps Herds to Raft Groups - Metadata Routing\"] end subgraph Finance_Herd[\"🏦 Finance Herd Partition\"] FinanceScheduler[\"Finance Scheduler (Leader) - DAG Planning - Placement Decisions\"] FinanceBarn[\"Finance Barn (KV Store) - BadgerDB (Local)\"] FinanceGovernance[\"Finance Governance Service - Lineage - Quality - Contracts\"] FinanceSecrets[\"Finance Secrets Manager - Raft-backed Secret Storage\"] FinanceRaft[\"Finance Raft Group (5 Nodes) (etcd-io/raft)\"] end subgraph Analytics_Herd[\"📊 Analytics Herd Partition\"] AnalyticsScheduler[\"Analytics Scheduler (Leader) - DAG Planning - Placement Decisions\"] AnalyticsBarn[\"Analytics Barn (KV Store) - BadgerDB (Local)\"] AnalyticsGovernance[\"Analytics Governance Service - Lineage - Quality - Contracts\"] AnalyticsSecrets[\"Analytics Secrets Manager - Raft-backed Secret Storage\"] AnalyticsRaft[\"Analytics Raft Group (5 Nodes) (etcd-io/raft)\"] end subgraph Worker_Cluster[\"🧱 Worker Nodes Cluster\"] RuniAgent[\"Runi Agent x100 - Node Registration - Slice Management - Metrics Collection\"] RuniSlice[\"Runi Slice (Ephemeral Container) - Herd Namespaced - Config Loaded - Secrets Injected\"] end Developer --\u003e | CLI/API Requests | GlobalAPI GlobalAPI --\u003e | Resolve Herd Assignment | HerdDirectory GlobalAPI --\u003e | Finance Pipelines | FinanceScheduler GlobalAPI --\u003e | Analytics Pipelines | AnalyticsScheduler FinanceScheduler --\u003e | DAG and Placement Reads | FinanceBarn FinanceGovernance --\u003e | Metadata/Lineage Writes | FinanceBarn FinanceSecrets --\u003e | Secrets CRUD | FinanceBarn FinanceBarn --\u003e | Log Replication | FinanceRaft AnalyticsScheduler --\u003e | DAG and Placement Reads | AnalyticsBarn AnalyticsGovernance --\u003e | Metadata/Lineage Writes | AnalyticsBarn AnalyticsSecrets --\u003e | Secrets CRUD | AnalyticsBarn AnalyticsBarn --\u003e | Log Replication | AnalyticsRaft FinanceScheduler --\u003e | Dispatch Finance Slices | RuniAgent AnalyticsScheduler --\u003e | Dispatch Analytics Slices | RuniAgent RuniAgent --\u003e | Launch with Herd Isolation | RuniSlice RuniAgent --\u003e | Fetch Finance Secrets | FinanceSecrets RuniAgent --\u003e | Fetch Analytics Secrets | AnalyticsSecrets RuniSlice --\u003e | Emit Lineage Events | FinanceGovernance RuniSlice --\u003e | Emit Lineage Events | AnalyticsGovernance RuniSlice --\u003e | Expose Service Port | RuniAgent RuniAgent --\u003e | Port-Forwarded Access | Developer 🛠️ Prerequisites Ensure you have:",
    "tags": [],
    "title": "Runink Quickstart",
    "uri": "/runink-site/docs/runink_quickstart/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Schema \u0026 Contract Management – Runink Runink enables data contracts as native Go structs — giving you strong typing, version tracking, schema validation, and backward compatibility across pipelines.\nThis guide shows how to define, version, test, and enforce schema contracts in your pipelines.\n📦 What Is a Contract? A contract in Runink is a schema definition used to:\nValidate incoming and outgoing data Detect schema drift Provide PII and RBAC tagging Drive pipeline generation and testing Contracts are generated from Go structs annotated with tags.\n✍️ Defining a Contract This schema contract defines the structure and policy metadata for incoming FDC3-based trade events. It ensures compliance with financial regulations and enables secure, testable transformations.\npackage contracts // ContractName: fdc3events // Version: 1.0.0 // Classification: pii // Compliance: SOX, GDPR, PCI-DSS // AccessPolicy: herd-isolated // SLO: 99.9% // Source: Kafka Stream (\"topics.trade_events\") type FDC3Event struct { TradeID string `json:\"trade_id\" validate:\"required\" pii:\"false\"` Symbol string `json:\"symbol\" validate:\"required\" pii:\"false\"` Price float64 `json:\"price\" validate:\"required\" pii:\"false\"` Timestamp string `json:\"timestamp\" validate:\"required\" pii:\"false\"` // PII Fields (Must be masked and tested) SSN string `json:\"ssn,omitempty\" pii:\"true\" access:\"compliance\"` BankAccount string `json:\"bank_account,omitempty\" pii:\"true\" access:\"finance\"` Email string `json:\"email,omitempty\" pii:\"true\" access:\"support\"` // Metadata for governance tracking Region string `json:\"region,omitempty\" lineage:\"true\"` Compliance []string `json:\"compliance_tags,omitempty\" lineage:\"true\"` Valid bool `json:\"valid,omitempty\"` } 🔐 Field-Level Tags Tag Description validate:\"required\" Required for schema validation pii:\"true\" Field contains sensitive personal data access:\"role\" RBAC-enforced visibility (e.g., support, finance) lineage:\"true\" Field tracked for lineage and audit logging 🧪 Decode Stage This function is the first @step in most .dsl scenarios using this contract:\n// DecodeFDC3Events parses raw CDM Kafka events into structured FDC3Event objects. func DecodeFDC3Events(r io.Reader, w io.Writer) error { decoder := json.NewDecoder(r) encoder := json.NewEncoder(w) for decoder.More() { var e FDC3Event if err := decoder.Decode(\u0026e); err != nil { return err } encoder.Encode(e) } return nil } ✅ Used In: features/fdc3_validation.dsl golden/cdm_trade/fdc3events.validated.golden.json fdc3events.conf as the runtime binding Then run:\nruni contract gen --struct contracts.Order --out contracts/order.json ✅ Enforcing a Contract Given the contract: contracts/order.json Or:\nruni run --verify-contract Runink ensures that all records match the expected schema.\n🔍 Schema Drift Detection Compare current vs expected schema:\nruni contract diff --old v1.json --new v2.json Output shows added, removed, or changed fields, types, tags, and ordering.\n📊 Hashing and Snapshotting Each contract has a hash for:\nVersion tracking Lineage graph integrity Change detection runi contract hash contracts/order.json Snapshot for reproducibility:\nruni snapshot --contract contracts/order.json --out snapshots/order_v1.json 🧬 Advanced Tags pii:\"true\" – marks field as sensitive access:\"finance\" – restricts field to roles enum:\"pending,approved,rejected\" – enum constraint (optional) required:\"true\" – fail if field is null or missing 🧪 Contract Testing Use golden tests to assert schema correctness:\nruni test --scenario features/orders.dsl And diff output against expected:\nruni diff --gold testdata/orders.golden.json --new out/orders.json 🗃️ Contract Catalog Generate an index of all contracts in your repo:\nruni contract catalog --out docs/contracts.json This can be plugged into:\nDocs browser Contract registry CI schema check 🧾 Example Contract Output { \"name\": \"Order\", \"fields\": [ { \"name\": \"order_id\", \"type\": \"string\" }, { \"name\": \"amount\", \"type\": \"float64\" }, { \"name\": \"notes\", \"type\": \"string\", \"pii\": true, \"access\": \"support\" } ], \"hash\": \"a94f3bc...\" } Summary Contracts in Runink power everything:\nSchema validation RBAC and compliance Pipeline generation Test automation Lineage and snapshots Use contracts to make your data:\nSafe Trustworthy Documented Governed",
    "description": "Schema \u0026 Contract Management – Runink Runink enables data contracts as native Go structs — giving you strong typing, version tracking, schema validation, and backward compatibility across pipelines.\nThis guide shows how to define, version, test, and enforce schema contracts in your pipelines.\n📦 What Is a Contract? A contract in Runink is a schema definition used to:\nValidate incoming and outgoing data Detect schema drift Provide PII and RBAC tagging Drive pipeline generation and testing Contracts are generated from Go structs annotated with tags.",
    "tags": [],
    "title": "Schema Contracts",
    "uri": "/runink-site/docs/schema-contracts/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines \u003e  Runink Docs",
    "content": "Supported Versions We currently support the latest major release of pipetool. Older versions may not receive security updates or patches.\nReporting a Vulnerability If you discover a security vulnerability, please do not open a public issue.\nInstead, contact us directly:\nEmail: security@yourdomain.org PGP Key: https://yourdomain.org/pgp.key (optional) We aim to respond to all security reports within 5 business days. All disclosures will be handled confidentially and professionally.\nDisclosure Process Vulnerability reported via email Maintainers investigate and validate the issue A patch is prepared and tested privately Coordinated disclosure timeline is agreed upon with reporter Advisory + patched release are published Hall of Fame We may credit contributors who report valid vulnerabilities in our release notes, changelogs, or SECURITY.md — with consent.\nThank you for helping make Runink safer for everyone!",
    "description": "Supported Versions We currently support the latest major release of pipetool. Older versions may not receive security updates or patches.\nReporting a Vulnerability If you discover a security vulnerability, please do not open a public issue.\nInstead, contact us directly:\nEmail: security@yourdomain.org PGP Key: https://yourdomain.org/pgp.key (optional) We aim to respond to all security reports within 5 business days. All disclosures will be handled confidentially and professionally.\nDisclosure Process Vulnerability reported via email Maintainers investigate and validate the issue A patch is prepared and tested privately Coordinated disclosure timeline is agreed upon with reporter Advisory + patched release are published Hall of Fame We may credit contributors who report valid vulnerabilities in our release notes, changelogs, or SECURITY.",
    "tags": [],
    "title": "Security \u0026 RBAC",
    "uri": "/runink-site/docs/security/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/runink-site/categories/index.html"
  },
  {
    "breadcrumb": "Runink: Declarative, Secure Data Pipelines",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/runink-site/tags/index.html"
  }
]
