[
{
	"uri": "http://localhost:1313/architecture/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Runink Architecture: Go/Linux Native Distributed Data Environment Self-sufficient, distributed environment for orchestrating and executing data pipelines using Go and Linux primitives. This system acts as the cluster resource manager and scheduler (replacing Slurm), provides Kubernetes-like logical isolation and RBAC, integrates data governance features, and ensures robust security and observability. It aims for high efficiency by avoiding traditional virtualization or container runtimes like Docker. Define a self-sufficient, distributed environment for orchestrating and executing data pipelines using Go and Linux primitives, with enhanced metadata capabilities designed to support standard data governance (lineage, catalog) AND future integration of LLM-generated annotations.\nBelow one can find how each components coexist within the same cluster:\nRunink Components API Server / RBAC: Enforces RBAC policies based on user/service account permissions within a target Herd. Cluster State Store: Explicitly stores Herd definitions and their associated resource quotas. Scheduler: Considers Herd-level quotas when making placement decisions. Secrets Manager: Access to secrets might be scoped by Herd. Data Governance: Metadata (lineage, annotations) can be tagged by or associated with the Herd it belongs to. Runi Agent: Receives the target Herd context when launching a slice and uses this information to potentially configure namespaces and apply appropriate cgroup limits based on Herd quotas. Runi Slice: A single instance of a pipeline step running as an isolated Worker Slice Process. Executes entirely within the logical boundary and resource constraints defined by its assigned Herd. Runi Pipes: Primarily used now for internal communication within the Runi Agent to capture logs/stdio from the Runi Slice it execs, rather than for primary data transfer between steps. Herd: A logical grouping construct, similar to a Kubernetes Namespace, enforced via RBAC policies and potentially mapped to specific sets of Linux namespaces managed by Agents. Provides multi-tenancy and team isolation. Quotas can be applied per Herd. Go Native \u0026amp; Linux Primitives: Core components written in Go, directly leveraging cgroups, namespaces (user, pid, net, mount, uts, ipc), pipes, sockets, and exec for execution and isolation. Self-Contained Cluster Management: Manages a pool of physical or virtual machines, schedules workloads onto them, and handles node lifecycle. Serverless Execution Model: Users define pipelines and resource requests; Runink manages node allocation, scheduling, isolation, scaling (by launching more slices), and lifecycle. Users are subject to quotas managed via cgroups. Security First: Integrated identity (OIDC), RBAC, secrets management, network policies, encryption in transit/rest. Data Governance Aware: Built-in metadata tracking, lineage capture, and support for quality checks. With extension for storage/management of rich data annotations (e.g., from LLMs). Rich Observability: Native support for metrics (Prometheus) and logs (Fluentd). üß† Programming Approaches: Why They Power Runink Runink‚Äôs architecture isn‚Äôt just Go-native ‚Äî it‚Äôs intentionally designed around a few low-level but high-impact programming paradigms. These concepts are what let Runink outperform containerized stacks, enforce security without overhead, and keep pipelines testable, composable, and fast. Runink takes a radically different approach to pipeline execution than traditional data platforms ‚Äî instead of running heavy containers, JVMs, or external orchestrators, Runink uses Go-native workers, Linux primitives like cgroups and namespaces, and concepts like data-oriented design and zero-copy streaming to deliver blazing-fast, memory-stable, and secure slices.\nBelow, we walk through the four core techniques and where they show up in Runink‚Äôs components.\nüîÑ 1. Functional Pipelines \u0026ldquo;Like talking how your data flow over high-level functions.\u0026rdquo;\nRunink\u0026rsquo;s .dsl compiles to Go transforms that behave like pure functions: they take input (usually via io.Reader), apply a deterministic transformation, and emit output (via io.Writer). There\u0026rsquo;s no shared mutable state, no side effects ‚Äî just clear dataflow.\nThis makes pipelines:\nComposable: steps can be reused across domains Testable: golden tests assert input/output correctness Deterministic: behavior doesn\u0026rsquo;t depend on cluster state ‚úÖ Why it matters: It brings unit testability and DAG clarity to data pipelines ‚Äî without needing a centralized scheduler or stateful orchestrator.\n2. Data-Oriented Design (DOD) \u0026ldquo;Design for the CPU, not the developer.\u0026rdquo;\nInstead of modeling data as deeply nested structs or objects, Runink favors flat, contiguous Go structs. This aligns memory layout with CPU cache lines and avoids heap thrashing. This is especially important for Runink‚Äôs slice execution and contract validation stages, where predictable access to batches of structs (records) matters.\nContracts are validated by scanning []struct batches in tight loops. Pointers and indirection are minimized for GC performance. Contracts power both validation and golden test generation. Use slices of structs over slices of pointers to enable CPU cache locality. Align field access with columnar memory usage if streaming transforms run across many rows. Preallocate buffers in Runi Agent‚Äôs slice execution path to avoid GC churn. Core Idea: Layout memory for how it will be accessed, not how it\u0026rsquo;s logically grouped. Runink‚Äôs slices often scan, validate, or enrich large batches of records ‚Äî so struct layout, batching, and memory predictability directly impact performance.\nApply DOD in Runink: Prefer flat structs over nested ones in contracts: // Better type User struct { ID string Name string Email string } // Avoid: nested fields unless necessary type User struct { Meta struct { ID string } Profile struct { Name string Email string } } In runi slice: Use sync.Pool for reusable buffers (especially JSON decoding). Pre-size buffers based on contract hints (e.g., maxRecords=10000). Avoid interface{} ‚Äî use generated structs via go/types or go:generate. Benefits: Better memory throughput, fewer allocations, and Go GC-friendliness under load. Use structs of arrays (SoA) or []User with preallocated slices in transformations. Minimize pointer indirection. Use value receivers and avoid *string, *int unless you need nil. Design transforms that operate in tight loops, e.g., for _, rec := range batch. Go structs are faster to iterate over than Python dictionaries or Java POJOs. Access patterns align with how CPUs fetch and cache data. Contract validation and transforms run over preallocated []struct batches, not heap-bound objects. üí° For Python/Java devs: Think of this like switching from dicts of dicts to NumPy-like flat arrays ‚Äî but in Go, with static types and no GC spikes.\n‚úÖ Why it matters: You get predictable memory use and cache-friendly validation at slice scale ‚Äî perfect for CPU-bound ETL or large-batch processing.\n3. Zero-Copy and Streaming Pipelines \u0026ldquo;Avoid full in-memory materialization ‚Äî process as the data flows.\u0026rdquo;\nInstead of []record ‚Üí transform ‚Üí []record, Runink pipelines follow stream ‚Üí transform ‚Üí stream ‚Äî minimizing allocations and maximizing throughput. Avoid unnecessary data marshaling or full deserialization. Rely on:\nTransforms consume from io.Reader and emit to io.Writer. Stages communicate via os.Pipe(), net.Pipe(), or chan Record for intra-slice streaming. Only materialize records when needed for validation or transformation. Intermediate results never fully materialize in memory. Core Idea: Instead of []Record -\u0026gt; Transform -\u0026gt; []Record, operate on streams of bytes or structs using io.Reader, chan Record, or even UNIX pipes between stages.\nRunink Optimizations: Use io.Reader ‚Üí Decoder ‚Üí Transform ‚Üí Encoder ‚Üí io.Writer chain.\nDesign step transforms like this:\nfunc ValidateUser(r io.Reader, w io.Writer) error { decoder := json.NewDecoder(r) encoder := json.NewEncoder(w) for decoder.More() { var user contracts.User if err := decoder.Decode(\u0026amp;user); err != nil { return err } if isValid(user) { encoder.Encode(user) } } return nil } For multi-stage slices, use os.Pipe():\nr1, w1 := os.Pipe() r2, w2 := os.Pipe() go Normalize(r0, w1) // input -\u0026gt; step 1 go Enrich(r1, w2) // step 1 -\u0026gt; step 2 go Sink(r2, out) // step 2 -\u0026gt; sink Benefits: Constant memory even for massive datasets. Backpressure: If downstream slows down, upstream blocks ‚Äî great for streaming (Kafka, etc.). Enables DLQ teeing: tee := io.MultiWriter(validOut, dlqSink). Uses io.Reader / io.Writer rather than buffering everything in memory. Transforms run as pipes between goroutines ‚Äî like UNIX but typed. Memory stays flat, predictable, and bounded ‚Äî even for 10M+ record streams. üí° For pandas/Spark devs: This is closer to generator pipelines or structured stream micro-batches, but with Go‚Äôs backpressure-aware channels and streaming codecs.\n‚úÖ Why it matters: You can process unbounded streams or 100GB batch files with a stable memory footprint ‚Äî and gain built-in backpressure and DLQ support.\n4. Declarative Scheduling with Constraint Propagation \u0026ldquo;Schedule via logic, not instructions.\u0026rdquo; The Herd and Runi Agent coordination already benefits from Raft-backed state, but push it further with affinity-aware, declarative scheduling:\nRunink doesn‚Äôt assign slices imperatively. It solves where to run things, based on:\nIsolation: @herd(\u0026quot;analytics\u0026quot;) Define resource constraints (e.g., @requires(cpu=2, memory=512Mi, label=‚Äúgpu‚Äù)) in .dsl. Placement: @affinity(colocate_with=\u0026quot;step:Join\u0026quot;) Propagate slice placement decisions through constraint-solving logic instead of imperative scheduling. Record constraints in the Raft-backed state store to enforce deterministic task placement. You can build this as a small DSL-on-DSL layer (e.g. @affinity(domain=\u0026quot;finance\u0026quot;, colocate_with=\u0026quot;step:JoinUsers\u0026quot;)).\nBenefit: Stronger determinism, replayability, and multi-tenant safety.\nCore Idea: Model placement as a set of constraints: affinity, herd quota, GPU needs, tenant isolation, etc. Let the scheduler solve the placement rather than being told where to run.\nRunink DSL Extension: In .dsl:\n@step(\u0026#34;RunLLMValidation\u0026#34;) @affinity(label=\u0026#34;gpu\u0026#34;, colocate_with=\u0026#34;step:ParsePDFs\u0026#34;) @requires(cpu=\u0026#34;4\u0026#34;, memory=\u0026#34;2Gi\u0026#34;) This can be compiled into metadata stored in the Raft-backed scheduler store.\nScheduler Logic (Pseudo-Go): type Constraints struct { CPU int Memory int Affinity string Colocate string HerdScope string } func ScheduleStep(stepID string, constraints Constraints) (NodeID, error) { candidates := filterByHerd(constraints.HerdScope) candidates = filterByResources(candidates, constraints.CPU, constraints.Memory) if constraints.Colocate != \u0026#34;\u0026#34; { candidates = colocateWith(candidates, constraints.Colocate) } if constraints.Affinity != \u0026#34;\u0026#34; { candidates = matchLabel(candidates, constraints.Affinity) } return pickBest(candidates) } [runi agent (PID 1 inside namespace)] | +--\u0026gt; Launch slice (Normalize step) |--\u0026gt; io.Pipe() Reader --\u0026gt; (Goroutine: Validate step) |--\u0026gt; io.Pipe() Reader --\u0026gt; (Goroutine: Enrich step) |--\u0026gt; Final Writer (sink to disk or message bus) These constraints are stored in the Raft-backed Barn and evaluated by the scheduler. In this sense, all decisions are Raft-logged, making slice scheduling auditable and replayable.\nüí° If you\u0026rsquo;re used to Kubernetes or Docker: Think of slices as ephemeral containers, but 10x faster ‚Äî no image pulls, no pod scheduling latency. No containers, no clusters ‚Äî just data pipelines that behave like code.\n‚úÖ Why it matters: Runink achieves multi-tenant safety, fault-tolerant execution, and reproducible placement ‚Äî without complex K8s YAML or retries.\nSummary Table Approach Use In Runink Why It Powers Runink Functional Pipelines .dsl ‚Üí Go transforms via @step() Clear transforms, reusable logic, golden testing Data-Oriented Design Contract enforcement, slice internals Memory locality, low-GC, CPU-efficient pipelines Zero-Copy Streaming Slice-to-slice transport, pipe-to-pipe steps Constant memory, streaming support, low latency Declarative Scheduling Herd quotas + slice placement affinity in .dsl raft store Deterministic, fair, replayable orchestration 5. Golden Testing and Contract-Aware Execution All pipelines are backed by Go structs as schema contracts:\nVersioned, diffable, validated at runtime Pipeline steps are tested using golden output and synthetic data generators Runink can detect schema drift and route invalid records to DLQ üí° This gives you the safety of DBT with the flexibility of code-first transforms.\nüìà Technology Benchmark: Go + Raft + Linux Primitives vs. JVM + Containerized Stacks Dimension Traditional (JVM / Container / DataFrame) Runink (Go / FP Pipelines / Raft) Gains Language Runtime Java Virtual Machine Go runtime ‚Ä¢Lower startup latency (milliseconds vs. seconds) ‚Ä¢ Smaller memory footprint per process (tens of MBs vs. hundreds of MBs) Programming Paradigm OOP (classes, inheritance, heavy type hierarchies) Functional-ish pipelines in Go (pure functions, composable stages) ‚Ä¢Simpler abstractions (no deep inheritance) ‚Ä¢ Easier reasoning about data flow ‚Ä¢ Higher testability via pure stage functions Data Processing Model Row-based DataFrames Channel-based streams of structs (Go channels ‚Üí functional transforms) ‚Ä¢Constant memory for streaming (no full in-memory tables) ‚Ä¢ Back-pressure and windowing naturally via channels ‚Ä¢ Composable, step-by-step transforms Isolation \u0026amp; Sandboxing Containers + VM overhead cgroups + namespaces + Go processes ‚Ä¢0.5‚Äì1 ms slice startup vs. 100 ms+ container spin-up ‚Ä¢ Fine-grained resource limits per slice ‚Ä¢ No dockerd overhead Inter-Stage Communication Shuffle via network storage or distributed file systems In-memory pipes, UNIX sockets, or gRPC streams within Go ‚Ä¢\u0026lt;1 ms handoff latency ‚Ä¢ Zero-copy possible with io.Pipe ‚Ä¢ Strong type safety end-to-end Distributed Consensus External etcd or no coordination Built-in Raft ‚Ä¢Linearizable consistency for control plane ‚Ä¢ Leader election and automatic recovery ‚Ä¢ Atomic updates across scheduler, secrets, and metadata Multi-Tenancy \u0026amp; Security K8s namespaces + complex RBAC, network policies Herd namespaces + cgroup quotas + OIDC/JWT + mTLS ‚Ä¢Single-layer isolation ‚Ä¢ Per-slice ephemeral UIDs for zero-trust ‚Ä¢ Simpler, declarative Herd-scoped policies Observability \u0026amp; Lineage External stacks Native agents + Raft-consistent metadata store ‚Ä¢Always-consistent lineage ‚Ä¢ Unified telemetry (metrics, logs, traces) ‚Ä¢ Real-time auditability without manual integration Deployment \u0026amp; Ops Helm charts, CRDs, multi-tool CI/CD Single Go binary + Raft cluster bootstrap ‚Ä¢One artifact for all services ‚Ä¢ Simpler upgrades via rolling Raft restarts ‚Ä¢ Built-in health \u0026amp; leader dashboards Functional Extensibility Plugins in Java/Python with JNI or UDFs (heavy) Go plugins or simple function registration ‚Ä¢No cross-language bridges ‚Ä¢ First-class Go codegen from DSL ‚Ä¢ Easier on-the-fly step injection Key Takeaways Speed \u0026amp; EfficiencyGo‚Äôs minimal runtime and direct use of Linux primitives deliver sub-millisecond task launches and minimal per-slice overhead‚Äîvs. multi-second container or JVM startups. Deterministic, Fault-Tolerant ControlRaft gives Runink a single, consistent source of truth across all core services‚Äîeliminating split-brain and eventual-consistency pitfalls that plague layered stacks. Composable, Functional PipelinesChannel-based, stage-oriented transforms allow lean, testable, streaming-first ETL‚Äîrather than bulk-loading entire tables in memory. Unified, Secure Multi-TenancyHerds + namespaces + cgroups + OIDC/JWT yield strong isolation and a simpler security model than stitching together K8s, Vault, and side-cars. Built-In Governance \u0026amp; Observability Raft-backed metadata ensures lineage and schema contracts are always in sync‚Äîno external governance platform required. By grounding Runink‚Äôs design in Go, functional pipelines, Linux primitives, and Raft consensus, you get a single, vertically integrated platform that outperforms and out-operates the conventional ‚ÄúJVM + containers + orchestration + governance‚Äù stack‚Äîdelivering predictable, low-overhead, and secure data pipelines at scale.\nüß† Raft-Backed Architecture At the heart of Runink\u0026rsquo;s distributed control plane is Raft consensus, which ensures strong consistency and availability across multiple replicas in a multi-tenant setup. Here‚Äôs how Raft plays a crucial role:\nCluster State Store (Barn): Raft ensures consistency between multiple replicas of the state store, preventing conflicts and enabling data to be synchronized across different services. With critical data, such as pipeline definitions, herd configurations, secrets, and RBAC policies, is replicated and stored in the Barn, which is under Raft consensus.\nWrites and Reads: All writes (e.g., task assignments, secrets, RBAC policies) go through the Raft leader to ensure consistency. If the leader fails, Raft elects a new leader to ensure continuous operation without data loss.\nScheduler: Uses Raft\u0026rsquo;s consensus for task scheduling to ensure that nodes in the cluster agree on which worker slices should be assigned the next task. Task assignments are synchronized across the cluster to avoid conflicts and duplications.\nSecrets Manager: Managed via Raft to ensure that secrets are consistently available across all nodes, with encrypted storage and strict RBAC. Guarantees that secrets are securely injected into Runi agents and worker slices at runtime, with access scoped per Herd.\nData Governance \u0026amp; Lineage: Lineage and metadata are tagged and tracked consistently across the cluster, with Raft-backed guarantees to prevent discrepancies in historical records.\nResilience \u0026amp; High Availability: Raft consensus enables Runink to maintain availability and consistency in the face of node failures by synchronizing state across multiple replicas. The API Server, Scheduler, Secrets Manager, and other components benefit from Raft to ensure high availability, disaster recovery, and distributed state consistency.\nWith Raft integrated into Runink, the system can operate fault-tolerantly, ensuring that data across the entire platform remains consistent, even when network partitions or node failures occur. This guarantees that your data pipelines, metadata, secrets, and workload scheduling are managed reliably in any cloud-native or distributed environment\nUser interaction Runink operates with a Control Plane managing multiple Worker Nodes, each running a Runi Agent.\nFuture LLM Integration Pipeline Definition: A user defines a pipeline step specifically for LLM annotation. This step specifies the input data source (e.g., path on shared FS/MinIO), the target LLM (e.g., OpenAI model name or internal service endpoint), the prompt, and potentially the output format. Scheduling: The Scheduler assigns this step to a Runi Agent. If targeting an internal LLM requiring specific hardware (GPU), the scheduler uses node resource information (reported by Agents) for placement. Execution: The Runi Agent launches a Worker Slice Process for this step. Credentials: The Worker Slice receives necessary credentials (e.g., OpenAI API key, MinIO access key) securely via the Secrets Manager. LLM Call: The worker reads input data, constructs the prompt, calls the relevant LLM API (external or internal), potentially handling batching or retries. Metadata Persistence: Upon receiving results, the worker extracts the annotations, formats them according to the Data Governance Service schema, and sends them via gRPC call to the service, linking them to the input data reference. It also reports standard lineage (input data -\u0026gt; LLM step -\u0026gt; annotations). Usage: Downstream pipeline steps or external users can then query the Data Governance Service (via API Server) to retrieve these annotations for further processing, reporting, or analysis. "
},
{
	"uri": "http://localhost:1313/benchmark/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "üìä Runink vs. Competitors: Raft-Powered Benchmark 1. Architecture \u0026amp; Paradigm Runink: A Go/Linux-native, vertically integrated data platform that combines execution, scheduling, governance, and observability in a single runtime. Unlike traditional stacks, Runink does not rely on Kubernetes or external orchestrators. Instead, it uses a Raft-based control plane to ensure high availability and consensus across services like scheduling, metadata, and security ‚Äî forming a distributed operating model purpose-built for data.\nCompetitors: Use a layered, loosely coupled stack:\nExecution: Spark, Beam (JVM-based) Orchestration: Airflow, Dagster (often on Kubernetes) Transformation: DBT (runs SQL on external data platforms) Cluster Management: Kubernetes, Slurm Governance: Collibra, Apache Atlas (external) Key Differentiator: Runink is built from the ground up as a distributed system ‚Äî with Raft consensus at its core ‚Äî whereas competitors compose multiple tools that communicate asynchronously or rely on external state systems.\n1. MapReduce vs. RDD vs. Raft for Data Pipelines 1.1 Architecture \u0026amp; Paradigm Aspect MapReduce RDD Raft (Runink model) Origin Google (2004) Spark (2010) Raft (2013) adapted for distributed control Execution Model Batch, two-stage (Map ‚Üí Reduce) In-memory DAGs of transformations Real-time coordination of distributed nodes Consistency Model Eventual (job outputs persisted) Best effort (job outputs in memory, lineage for recovery) Strong consistency (N/2+1 consensus) Primary Use Large batch analytics Interactive, iterative analytics Distributed metadata/state management for pipelines Fault Tolerance Output checkpointing Lineage-based recomputation Log replication and state machine replication 1.2. Performance \u0026amp; Efficiency Aspect MapReduce RDD Raft (Runink) Cold Start Time High (JVM startup, slot allocation) Medium (Spark cluster overhead) Low (Go processes, native scheduling) Memory Use Disk-heavy Memory-heavy (RDD caching) Lightweight (control metadata, not bulk data) I/O Overhead Heavy disk I/O (HDFS reads/writes) Network/memory optimized, but needs enough RAM Minimal (only metadata replication) Pipeline Complexity Requires multiple jobs for DAGs Natural DAG execution Direct DAG compilation from DSLs (Runink) 1.3. Data Governance and Lineage Aspect MapReduce RDD Raft (Runink) Built-in Lineage No (external) Yes (RDD lineage graph) Yes (atomic commit of contracts, steps, runs) Governance APIs Manual (logs, job output) Partial (Spark listeners) Native (contracts, lineage logs, per-slice metadata) Auditability Hard to reconstruct Possible with effort Native per-run audit logs, Raft-signed events 1.4. Fault Tolerance and Recovery Aspect MapReduce RDD Raft (Runink) Recovery Mechanism Re-run failed jobs Recompute from lineage Replay committed log entries Failure Impact Full-stage re-execution Depends on lost partitions Minimal if quorum is maintained Availability Guarantee None Partial (driver failure = job loss) Strong (as long as majority nodes are alive) 1.5. Security and Isolation Aspect MapReduce RDD Raft (Runink) Authentication Optional Optional Mandatory (OIDC, RBAC) Secrets Management Ad hoc Ad hoc Native, Raft-backed, scoped by Herds Multi-Tenancy None None Herd isolation (namespace + cgroup enforcement) 1.6 Real case scenario example Imagine a critical pipeline for trade settlement:\nMapReduce would force every job to write to disk between stages ‚Äî slow and painful for debugging. RDD would speed things up but require heavy RAM and still risk full job loss if the driver fails. Raft (Runink) keeps every contract, every transformation, every secret atomically committed and recoverable ‚Äî even if a node crashes mid-run, the system can resume from the last committed stage safely. 2. Raft Advantages for Distributed Coordination Runink:\nUses Raft for strong consistency and leader election across:\nControl plane state (Herds, pipelines, RBAC, quotas) Scheduler decisions Secrets and metadata governance Guarantees:\nNo split-brain conditions Predictable and deterministic behavior in failure scenarios Fault-tolerant HA (N/2+1 consensus) Competitors:\nKubernetes uses etcd (Raft-backed), but tools like Airflow/Spark have no equivalent. Scheduling decisions, lineage, and metadata handling are often eventually consistent or stored in external systems without consensus guarantees. Result: higher complexity, latency, and coordination failure risks under scale or failure. 3. Performance \u0026amp; Resource Efficiency Runink:\nWritten in Go for low-latency cold starts and efficient concurrency. Uses direct exec, cgroups, and namespaces, not Docker/K8s layers. Raft ensures low-overhead coordination, avoiding polling retries and state divergence. Competitors:\nSpark is JVM-based; powerful but resource-heavy. K8s introduces orchestration latency, plus pod startup and scheduling delays. Airflow relies on Celery/K8s executors with less efficient scheduling granularity. 4. Scheduling \u0026amp; Resource Management Runink:\nCustom, Raft-backed Scheduler matches pipeline steps to nodes in real time. Considers Herd quotas, CPU/GPU/Memory availability. Deterministic task placement and retry logic are logged and replayable via Raft. Competitors:\nKubernetes schedulers are general-purpose and not pipeline-aware. Airflow does not control actual compute ‚Äî delegates to backends like K8s. Slurm excels in HPC, but lacks pipeline-native orchestration and data governance. 5. Security Model Runink:\nSecure-by-default with OIDC + JWT, RBAC, Secrets Manager, mTLS, and field-level masking. Secrets are versioned and replicated with Raft, avoiding plaintext spillage or inconsistent states. Namespace isolation per Herd. Competitors:\nKubernetes offers RBAC and secrets, but complexity leads to misconfigurations. Airflow often shares sensitive configs (connections, variables) across DAGs. 6. Data Governance, Lineage \u0026amp; Metadata Runink:\nBuilt-in Data Governance Service stores contracts, lineage, quality metrics, and annotations. Changes are committed to Raft, ensuring atomic updates and rollback support. Contracts and pipeline steps are versioned and tracked centrally. Competitors:\nRequire integrating platforms like Atlas or Collibra. Lineage capture is manual or partial, with data loss possible on failure or drift. Metadata syncing lacks consistency guarantees. 7. Multi-Tenancy Runink:\nUses Herds as isolation units ‚Äî enforced via RBAC, ephemeral UIDs, cgroups, and namespace boundaries. Raft ensures configuration updates (quotas, roles) are safely committed across all replicas. Competitors:\nKubernetes uses namespaces and resource quotas. Airflow has no robust multi-tenancy ‚Äî teams often need separate deployments. 8. LLM Integration \u0026amp; Metadata Handling Runink:\nLLM inference is a first-class pipeline step. Annotations are tied to lineage and stored transactionally in the Raft-backed governance store. Competitors:\nLLMs are orchestrated as container steps via KubernetesPodOperator or Argo. Metadata is stored in external tools or left untracked. 9. Observability Runink:\nBuilt-in metrics via Prometheus, structured logs via Fluentd. Metadata and run stats are Raft-consistent, enabling reproducible audit trails. Observability spans from node ‚Üí slice ‚Üí Herd ‚Üí run. Competitors:\nSpark, Airflow, and K8s use external stacks (Loki, Grafana, EFK) that need configuration and instrumentation. Logs may be disjointed or context-lacking. 10. Ecosystem \u0026amp; Maturity Runink:\nEarly-stage, but intentionally narrow in scope and highly integrated. No need for external orchestrators or data governance platforms. Competitors:\nVast ecosystems (Airflow, Spark, DBT, K8s) with huge community support. Tradeoff: Requires significant integration, coordination, and DevOps effort. 11. Complexity \u0026amp; Operational Effort Runink:\nHigh initial build complexity ‚Äî but centralization of Raft and Go-based primitives allows for deterministic ops, easier debug, and stronger safety guarantees. Zero external dependencies once deployed. Competitors:\nOperationally fragmented. DevOps teams must manage multiple platforms (e.g., K8s, Helm, Spark, Airflow). Requires cross-tool observability, secrets management, and governance. ‚úÖ Summary: Why Raft Makes Runink Different Capability Runink (Raft-Powered) Spark / Airflow / K8s Stack State Coordination Raft Consensus Partial (only K8s/etcd) Fault Tolerance HA Replication Tool-dependent Scheduler Raft-backed, deterministic Varies per layer Governance Native, consistent, queryable External Secrets Encrypted + Raft-consistent K8s or env vars Lineage Immutable + auto-tracked External integrations Multitenancy Herds + namespace isolation Namespaces (K8s) Security End-to-end mTLS + RBAC + UIDs Complex setup LLM-native First-class integration Ad hoc orchestration Observability Built-in, unified stack Custom integration Absolutely ‚Äî let‚Äôs break down how Runink‚Äôs architecture aligns Go-bound/unbound slices with Linux primitives, all coordinated by Raft, and how this model compares favorably to industry-standard stacks like Kubernetes, Apache Spark, and Airflow.\nüß© Go Slices, Linux Primitives, and Raft: Runink‚Äôs Execution Philosophy üîç Overview Runink executes data pipelines using Go \u0026ldquo;slices\u0026rdquo; ‚Äî lightweight, isolated execution units designed to model both bounded (batch) and unbounded (streaming) data workloads. These are:\nSpawned by the Runi agent on worker nodes Executed as isolated processes Scoped to Herd namespaces Constrained by cgroups Communicate via pipes, sockets, or gRPC streams This orchestration is Raft-coordinated, making every launch deterministic, fault-tolerant, and observable.\nüß¨ What Are Bounded and Unbounded Slices? Type Use Case Description Bounded Batch ETL, contract validation Processes a finite dataset and terminates Unbounded Streaming ingestion, log/event flows Long-running, backpressured pipelines with checkpointing Both types run as Go processes within a controlled Herd namespace, and can be composed together in DAGs.\nüß∞ Slice Internals: Go + Linux Synergy Each slice is a native Go process managed via:\n‚úÖ Cgroups Applied per Herd, per slice Limits on CPU, memory, I/O Enforced using Linux cgroupv2 hierarchy Supports slice preemption and fair resource sharing ‚úÖ Namespaces User, mount, network, and PID namespaces Enforce isolation between tenants (Herds) Prevent noisy-neighbor problems and info leaks ‚úÖ Pipes \u0026amp; IPC Use of os.Pipe() or io.Pipe() in Go to model stage-to-stage communication net.Pipe() and UNIX domain sockets for local transport Optionally enhanced via io.Reader, bufio, or gRPC streams (for cross-node slices) ‚úÖ Execution os/exec with setns(2) and clone(2) to launch each slice Environment-injected config and secrets fetched securely via Raft-backed Secrets Manager üîÑ Raft as Execution Backbone The Barn (Cluster State Store) is Raft-backed. It ensures:\nRaft Role Benefit to Runink Leader Election Prevents race conditions in pipeline launches Log Replication Guarantees all agents/schedulers share same DAG, lineage, and config Strong Consistency Execution decisions are deterministic and audit-traceable Fault Tolerance Node crashes do not corrupt state or duplicate work Examples of Raft-integrated flows:\nDAG submission is a Raft log entry Herd quota changes update slice scheduling state DLQ routing is replicated for contract validation violations Slice termination is consensus-driven (no orphaned processes) üöÄ How This Model Beats the Status Quo ‚úÖ Compared to Apache Spark Spark (JVM) Runink (Go + Linux primitives) JVM-based, slow cold starts Instantaneous slice spawn using exec Containerized via YARN/Mesos/K8s No container daemon needed Fault tolerance via RDD lineage/logs Strong consistency via Raft Needs external tools for lineage Built-in governance and metadata ‚úÖ Compared to Kubernetes + Airflow Kubernetes / Airflow Runink DAGs stored in SQL, not consistent across API servers DAGs submitted via Raft log, replicated to all Task scheduling needs K8s Scheduler or Celery Runi agents coordinate locally via consensus Containers = overhead Direct exec in a namespaced PID space Secrets are environment or K8s Secret dependent Raft-backed, RBAC-scoped Secrets Manager Governance/logging external Observability and lineage native and real-time üîê Security Bonus: Ephemeral UIDs \u0026amp; mTLS Each slice runs as:\nA non-root ephemeral user With an Herd-specific UID/GID In an isolated namespace Authenticated over mTLS via service tokens This makes it:\nSafer than Docker containers with root mounts More auditable than shared Airflow workers Easier to enforce per-tenant quotas and access policies üìà Use Case Amplification Scenario Runink Edge Contract enforcement + DLQ routing Fault-tolerant validation with Raft-backed retries LLM prompt + response execution LLM step tracked + annotated in pipeline DAG Streaming analytics across partitions Unbounded slices backpressured with state checkpoints Batch SQL + post-transform validation Sequential Go slices piped and DAG-aware üß† Conclusion: Go + Linux internals + Raft = Data-Native Compute Runink leverages Raft consensus not just for fault tolerance, but as a foundational architectural choice. It eliminates whole categories of orchestration complexity, state drift, and configuration mismatches by building from first principles ‚Äî while offering a single runtime that natively understands pipelines, contracts, lineage, and compute.\nIf you‚Äôre designing a modern data platform ‚Äî especially one focused on governance, and efficient domain isolation ‚Äî Runink is a radically integrated alternative to the Kubernetes-centric model.\nProcess flow: Runink:\nUses Go slices instead of containers Wraps execution with kernel primitives (namespaces, cgroups, pipes) Orchestrates them with Raft-consistent DAGs Tracks everything via native Governance/Observability layers This makes Runink:\nFaster to start More resource-efficient than JVM- or container-based runners Deterministic and fault-tolerant with Raft consensus Governance-native, with automatic metadata and lineage capture Competitors piece together these guarantees from Kubernetes, Airflow, Spark, and Collibra. Runink offers them by design.\n"
},
{
	"uri": "http://localhost:1313/cli/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "üß∞ Runink CLI Reference (runi) The runi CLI is the command-line interface to everything in the Runink data platform. It‚Äôs your developer-first companion for defining, testing, running, securing, and publishing data pipelines ‚Äî all from declarative .dsl files and Go-native contracts.\nThis reference describes all available commands, grouped by capability.\nüß± Project \u0026amp; Pipeline Lifecycle Command Description runi herd init [project-name] Scaffold a new workspace with starter contracts, features, CI config runi compile --scenario \u0026lt;file\u0026gt; Generate Go pipeline code from .dsl files runi run --scenario \u0026lt;file\u0026gt; --contract \u0026lt;contract.json\u0026gt; Run pipelines locally or remotely runi watch --scenario \u0026lt;file\u0026gt; Auto-compile \u0026amp; re-run scenario on save üìÅ Schema \u0026amp; Contract Management Command Description runi contract gen --struct \u0026lt;pkg.Type\u0026gt; Generate a contract from Go struct runi contract diff --old v1.json --new v2.json Show schema drift between versions runi contract rollback Revert to previous contract version runi contract history --name \u0026lt;contract\u0026gt; Show all versions and changelog entries runi contract validate --file \u0026lt;file\u0026gt; Validate a file against a contract runi contract catalog Create an index of all contracts in the repo runi contract hash Generate contract hash for versioning üß™ Testing \u0026amp; Data Validation Command Description runi test --scenario \u0026lt;file\u0026gt; Run tests using golden files runi synth --contract \u0026lt;contract.json\u0026gt; Generate synthetic golden test data runi verify-contract --contract \u0026lt;file\u0026gt; Validate pipeline against contract üîê Security, Publishing \u0026amp; Compliance Command Description `runi secure [\u0026ndash;sbom \u0026ndash;sign runi publish Push metadata, lineage, and contracts to registry runi sbom export [--format spdx] Export SPDX-compliant software bill of materials runi changelog gen Generate changelogs from contract/feature diffs üîç Observability \u0026amp; Lineage Command Description runi lineage --run-id \u0026lt;uuid\u0026gt; Show DAG lineage for a run runi lineage track --source A --sink B Manually link lineage metadata runi lineage graph --out file.dot Export lineage graph in DOT format runi metadata get --key \u0026lt;name\u0026gt; Retrieve stored metadata for a step runi metadata annotate --key \u0026lt;name\u0026gt; Attach annotation to pipeline metadata runi logs --run-id \u0026lt;uuid\u0026gt; View logs for a specific run runi status --run-id \u0026lt;uuid\u0026gt; Check status of a pipeline execution ü§ñ Distributed Execution (Remote) Command Description `runi deploy \u0026ndash;target \u0026lt;k8s bigmachine\u0026gt;` runi start --slice \u0026lt;file\u0026gt; --herd \u0026lt;namespace\u0026gt; Start execution of a scenario remotely runi kill --run-id \u0026lt;uuid\u0026gt; Terminate running scenario üí™ Control Plane \u0026amp; Agents Command Description runi herdctl create Create a new Herd (namespace + quotas + policies) runi herdctl delete Delete a Herd runi herdctl update Update Herd quotas, RBAC, metadata runi herdctl list List all Herds and resource states runi herdctl quota set \u0026lt;herd\u0026gt; Update CPU/mem quotas live runi herdctl lineage \u0026lt;herd\u0026gt; View lineage graphs scoped to a Herd runi agentctl list List active Runi agents, resource usage, labels runi agentctl status \u0026lt;agent\u0026gt; Detailed agent status (health, registered slices, metrics) runi agentctl drain \u0026lt;agent\u0026gt; Mark agent as unschedulable (cordon) runi agentctl register Manually register agent (optional bootstrap) runi agentctl cordon \u0026lt;agent\u0026gt; Prevent slice scheduling üåê Worker Slice Management Command Description runi slicectl list --herd \u0026lt;id\u0026gt; List all active slices for a Herd runi slicectl logs \u0026lt;slice-id\u0026gt; Fetch logs for a given slice runi slicectl cancel \u0026lt;slice-id\u0026gt; Cancel a running slice gracefully runi slicectl metrics \u0026lt;slice-id\u0026gt; Show real-time metrics for a slice runi slicectl promote \u0026lt;slice-id\u0026gt; Checkpoint a slice mid-run üîÄ Introspection \u0026amp; Visualization Command Description runi explain --scenario \u0026lt;file\u0026gt; Describe DAG and step resolution logic runi graphviz --scenario \u0026lt;file\u0026gt; Render DAG as a .png, .svg, or .dot runi diff --feature old.dsl --feature new.dsl Compare feature files and show logic drift üß™ REPL \u0026amp; Exploratory Commands Command Description runi repl Launch interactive DataFrame, SQL, JSON REPL runi json explore -f file.json -q '.email' Run jq-style query on JSON runi query -e \u0026quot;SELECT * FROM dataset\u0026quot; Run SQL-like query on scenario input üõ†Ô∏è Dev Tools \u0026amp; Generators Command Description runi gen --dsl input.json Generate feature from sample input runi contract from-feature \u0026lt;file\u0026gt; Extract contract from .dsl spec runi schema hash Generate contract fingerprint runi bump Auto-increment contract version with changelog üßπ Plugins \u0026amp; Extensions Command Description runi plugin install \u0026lt;url\u0026gt; Install external plugin runi plugin list List installed extensions runi plugin run \u0026lt;name\u0026gt; Execute a plugin subcommand üì¶ Packaging \u0026amp; CI/CD Command Description runi build Compile pipeline bundle for remote use runi pack Zip workspace for deployment/distribution runi upgrade Self-update the CLI and plugins runi doctor Diagnose CLI and project setup üìÖ Runtime Lifecycle Command Description runi restart --run-id \u0026lt;uuid\u0026gt; Restart a pipeline from last successful checkpoint runi resume --run-id \u0026lt;uuid\u0026gt; Resume paused pipeline without reprocessing runi checkpoint --scenario \u0026lt;file\u0026gt; Create a persistent step-based checkpoint marker üí¨ Collaboration \u0026amp; Governance Command Description runi comment --contract \u0026lt;file\u0026gt; Leave inline comments for review (contract-level QA) runi request-approval --contract \u0026lt;file\u0026gt; Submit contract for governance approval runi feedback --scenario \u0026lt;file\u0026gt; Attach review notes to a scenario üõ°Ô∏è Privacy, Redaction \u0026amp; Data Escrow Command Description runi redact --contract \u0026lt;file\u0026gt; Automatically redact PII based on tags runi escrow --run-id \u0026lt;uuid\u0026gt; Encrypt pipeline outputs for future unsealing runi anonymize --input \u0026lt;file\u0026gt; Generate synthetic version of a sensitive input file üóì Event-Based Execution Command Description `runi trigger \u0026ndash;on \u0026lt;webhook s3 runi listen --event \u0026lt;type\u0026gt; Listen for external event to start scenario runi subscribe --stream \u0026lt;source\u0026gt; Subscribe to stream source with offset recovery üîÑ Pipeline \u0026amp; Contract Lifecycle Command Description runi freeze --scenario \u0026lt;file\u0026gt; Lock DAG hash and contract state as immutable runi archive --herd \u0026lt;name\u0026gt; --keep \u0026lt;N\u0026gt; Archive old scenarios/runs beyond retention policy runi retire --contract \u0026lt;file\u0026gt; Deprecate contract from active use üß¨ Metadata Graph \u0026amp; Semantic Search Command Description runi knowledge export --format turtle Export contract and DAG metadata as RDF runi query lineage Run SQL-style queries across lineage metadata üß™ Experimental / LLM-integrated Command Description runi openai audit --contract \u0026lt;file\u0026gt; Use LLM to summarize or describe schema changes runi sandbox --scenario \u0026lt;file\u0026gt; Run pipeline in ephemeral namespace sandbox runi simulate --input \u0026lt;file\u0026gt; --window 5m Replay stream input over a time window runi mint-token --role \u0026lt;name\u0026gt; Mint temporary JWT token scoped to herd/contract üí¨ Use runi \u0026lt;command\u0026gt; --help for flags, options, and examples.\nRunink\u0026rsquo;s CLI gives you a full stack data engine in your terminal ‚Äî from contracts to clusters, from .dsl to full observability.\n"
},
{
	"uri": "http://localhost:1313/clihelp/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "üÜò Runink CLI: Help Template This is a developer-friendly help template for implementing consistent runi \u0026lt;command\u0026gt; --help outputs.\nüß± Format: Basic Help Command runi \u0026lt;command\u0026gt; [subcommand] [flags] Usage: runi \u0026lt;command\u0026gt; [options] Options: -h, --help Show this help message and exit -v, --verbose Show detailed logs and diagnostics Example: runi init --help Initialize a new Runink project. Usage: runi init [project-name] Flags: -h, --help Show help for init üîÑ Example: runi compile --help runi compile --scenario \u0026lt;file.dsl\u0026gt; Description: Compile a `.dsl` scenario and its contract into an executable Go DAG. Generates a Go file under `rendered/` based on contract-linked step tags. Usage: runi compile --scenario features/trade_cdm.dsl Flags: --scenario Path to a DSL scenario file --out Optional: custom output path for DAG (default: rendered/\u0026lt;name\u0026gt;.go) --dry-run Only validate scenario and contract, do not write DAG --verbose Show full DAG step resolution logs üß™ Example: runi test --help runi test --scenario \u0026lt;file.dsl\u0026gt; Description: Execute a feature scenario with golden test inputs and compare output. Supports diff mode and golden update flows. Usage: runi test --scenario features/onboard.dsl Flags: --scenario DSL file to test --golden Optional: override path to golden test folder --update Automatically update golden output on success --only \u0026lt;step\u0026gt; Run test up to a specific pipeline step üîê Example: runi contract gen --help runi contract gen --struct \u0026lt;package.Type\u0026gt; --out \u0026lt;file\u0026gt; Description: Generate a JSON contract definition from a Go struct. Includes schema, access tags, and validation metadata. Usage: runi contract gen --struct contracts.Customer --out contracts/customer.json Flags: --struct Fully qualified Go type (e.g. contracts.Customer) --out Output contract file path --flatten Inline nested types into flat fields --herd Optional: attach to specific herd (e.g. finance) runi contract diff --help Diff two versions of a contract and show schema drift. Usage: runi contract diff --old v1.json --new v2.json runi run --help Run a compiled pipeline with data inputs. Usage: runi run --scenario \u0026lt;file.dsl\u0026gt; [--contract file] [--herd name] Flags: --scenario Scenario to execute --contract Optional explicit contract --herd Herd to run pipeline in --dry-run Preview DAG resolution only runi lineage --help Show lineage metadata for a run. Usage: runi lineage --run-id \u0026lt;id\u0026gt; Flags: --run-id Unique run identifier --output Format (json|csv|graph) runi publish --help Publish contracts, lineage, and tags to metadata registry. Usage: runi publish --herd \u0026lt;name\u0026gt; [--scenario file] runi repl --help Start interactive REPL for querying test inputs or contract data. Usage: runi repl --scenario \u0026lt;path\u0026gt; ü§ñ Example: runi deploy --help runi deploy --target \u0026lt;target\u0026gt; Description: Deploy Runi workers and slices to a remote orchestration cluster. Usage: runi deploy --target k8s Flags: --target Target platform (k8s, bigmachine) --herd Herd (namespace) to deploy into --dry-run Simulate deployment without applying --confirm Require manual confirmation for remote changes runi schedule --help Schedule a pipeline scenario for recurring execution. Usage: runi schedule --scenario \u0026lt;file\u0026gt; --cron \u0026#34;0 6 * * *\u0026#34; Flags: --scenario DSL file --cron Cron-style expression runi audit --help Show schema contract change history and approvals. Usage: runi audit --contract \u0026lt;file\u0026gt; runi restart --help Restart a failed or incomplete pipeline run from its last checkpoint. Usage: runi restart --run-id \u0026lt;uuid\u0026gt; Flags: --run-id Run ID to restart from --force Ignore checkpoint and rerun from start runi resume --help Resume an interrupted or paused pipeline. Usage: runi resume --run-id \u0026lt;uuid\u0026gt; runi checkpoint --help Create a DAG state checkpoint for partial run recovery. Usage: runi checkpoint --scenario \u0026lt;file\u0026gt; runi comment --help Leave inline comments for contracts or fields (used in review tools). Usage: runi comment --contract \u0026lt;file\u0026gt; --field \u0026lt;path\u0026gt; --note \u0026lt;text\u0026gt; runi request-approval --help Submit a contract for governance approval and audit. Usage: runi request-approval --contract \u0026lt;file\u0026gt; runi feedback --help Attach feedback note to a scenario feature for peer review. Usage: runi feedback --scenario \u0026lt;file\u0026gt; --note \u0026lt;text\u0026gt; runi redact --help Automatically redact fields marked pii:\u0026#34;true\u0026#34; in a contract schema. Usage: runi redact --contract \u0026lt;file\u0026gt; --out \u0026lt;file\u0026gt; runi escrow --help Encrypt and store output data for delayed release or approval. Usage: runi escrow --run-id \u0026lt;uuid\u0026gt; --out \u0026lt;vault.json\u0026gt; runi anonymize --help Create a non-sensitive version of input using faker + tags. Usage: runi anonymize --input \u0026lt;file\u0026gt; --contract \u0026lt;file\u0026gt; --out \u0026lt;file\u0026gt; runi trigger --help Define an event trigger for this scenario. Usage: runi trigger --scenario \u0026lt;file\u0026gt; --on webhook|s3|pubsub runi listen --help Start a listener to monitor incoming event and dispatch pipeline. Usage: runi listen --event \u0026lt;type\u0026gt; runi subscribe --help Subscribe to a streaming topic or channel with offset tracking. Usage: runi subscribe --stream \u0026lt;topic\u0026gt; --window 5m runi freeze --help Freeze contract + scenario versions with hashes for snapshot validation. Usage: runi freeze --scenario \u0026lt;file\u0026gt; runi archive --help Archive old versions of scenarios and their runs by herd. Usage: runi archive --herd \u0026lt;name\u0026gt; --keep 3 runi retire --help Retire a contract so it cannot be used in future scenarios. Usage: runi retire --contract \u0026lt;file\u0026gt; runi lineage graph --help Export full DAG and contract lineage as GraphViz dot file. Usage: runi lineage graph --out lineage.dot runi knowledge export --help Export pipeline metadata using RDF serialization (Turtle/N-Triples). Usage: runi knowledge export --format turtle runi query lineage --help Query lineage metadata using SQL-like syntax. Usage: runi query lineage --sql \u0026#34;SELECT * WHERE pii = true\u0026#34; runi openai audit --help Use an LLM to summarize contract diffs or suggest field comments. Usage: runi openai audit --contract \u0026lt;file\u0026gt; runi sandbox --help Execute scenario in a secure ephemeral environment. Usage: runi sandbox --scenario \u0026lt;file\u0026gt; runi simulate --help Replay input data as a stream window to test stateful logic. Usage: runi simulate --input \u0026lt;file\u0026gt; --window 5m runi mint-token --help Generate a short-lived JWT for scoped access by herd or scenario. Usage: runi mint-token --herd finance --role analyst --ttl 5m üß† Best Practices ‚úÖ Describe what the command does, not how it\u0026rsquo;s implemented ‚úÖ Include at least 1 usage example ‚úÖ Use consistent flags: --scenario, --contract, --out, --herd ‚úÖ Provide guidance for --dry-run, --verbose, --help ‚úÖ Include multi-step examples if command touches multiple files "
},
{
	"uri": "http://localhost:1313/components/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Runink Component Reference This document describes each component of the Runink architecture, providing clear definitions, roles, and interactions within the overall system.\n‚öôÔ∏è Runink CLI Role: Developer interface for scaffolding, testing, and running pipelines.\nThe Runink CLI (runi) is the developer-first control surface for the entire data lifecycle ‚Äî from contracts and pipelines to tests, observability, and deployment. It acts as the orchestration layer for humans, offering a simple, scriptable, and powerful interface for everything Runink can do.\nWhether you\u0026rsquo;re scaffolding a new project, compiling .dsl files into Go pipelines, validating schema contracts, testing with golden files, or pushing artifacts to a metadata registry ‚Äî runi gives you the tools to do it fast and reliably, right from your terminal.\nFunctions:\nProject Scaffolding for quickly bootstrapping new repositories with features, contracts, tests, and CI configs Contract Management including generation from Go structs, diffing, rollback, and drift tracking Pipeline Compilation from .dsl scenarios into DAG-aware Go code with batch and streaming support Golden Testing \u0026amp; Synthetic Data using built-in Faker-based generation and contract-backed verification Metadata Publishing to push lineage, changelogs, and SBOMs to registries or dashboards Interactive REPL \u0026amp; Exploration for debugging, jq-style filtering, DataFrame operations, and live SQL queries Security \u0026amp; DevOps integrations for SBOM, artifact signing, and secure publishing pipelines For a full list of commands and examples, see docs/cli.md.\nThe CLI embeds core functionality like the Pipeline Generator, Execution Engine, and Governance Layer, turning complex distributed operations into single, declarative, CLI-driven workflows.\nWith runi, developers get a portable, scriptable, and audit-friendly control plane ‚Äî no YAML, no drag-and-drop UIs, just pipelines that flow from contract to cluster with confidence.\nü§ñ Runi Agent Runi (the Runink Agent) runs on every compute node, bridging the Control Plane and the isolated worker slices. It registers node resources and health, fetches secrets, and manages the lifecycle of pipeline steps within lightweight, cgroup‚Äëisolated ‚Äúslices.‚Äù Runi ensures that each step executes in the proper Herd context, collects observability data, and enforces resource quotas.\nFunctions:\nNode Registration with Runink Control Plane (health, capacity, labels) Slice Manager that creates cgroups and namespaces per Herd, launches worker processes, and applies resource limits Secrets Fetching via secure Raft‚Äëbacked Secrets Manager Observability Forwarding metrics to Prometheus, logs to Fluentd, traces to OpenTelemetry Lifecycle Management handles retries, restarts, graceful shutdowns, and cleanup Runi turns each compute node into a secure, observed, and policy‚Äëenforced worker ‚Äî ready to run any pipeline slice on demand.\nüêë Herd Namespace Herd Namespaces is Runink‚Äôs lightweight domain isolation layer, leveraging Linux namespaces to enforce shared‚Äënothing principles. Each ‚Äúherd‚Äù represents a logical group of related pipeline steps or data slices, isolated in its own namespace to guarantee resource control, security boundaries, and reproducibility.\nFunctions:\nDomain Isolation: Creates per‚Äëherd namespaces so pipelines for different domains (e.g., finance, ecommerce) cannot interfere Resource Control: Assigns cgroups for CPU, memory, and I/O limits on each herd‚Äôs worker slices Secure Multi‚ÄëTenancy: Ensures data and processes are confined to their designated namespaces Dynamic Scaling Spins up and tears down herds on demand, coordinating with bigmachine for serverless‚Äëstyle compute Observability Hooks Exposes namespace‚Äëscoped metrics and logs, tagged by herd ID and domain Multi‚ÄëTenant Governance separate catalogs, DLQs, and audit trails per Herd With Herd, Runink achieves fine‚Äëgrained isolation for domain-driven workloads ‚Äî combining security, performance, and multi‚Äëtenant governance in a single, native Linux construct. Empowering teams to collaborate on a shared platform without stepping on each other‚Äôs toes.\nüñ•Ô∏è Herd Control Plane Herd Control Plane is the high‚Äëavailability control plane that glues together pipelines, policies, and metadata. It serves as the central orchestrator and gateway for all user and agent interactions, enforcing authentication, authorization, and multi‚Äëtenant (Herd) scoping. Runink stores and manages the cluster state, schedules work, and provides metadata and governance APIs ‚Äî all over secure, TLS‚Äëencrypted gRPC/REST endpoints.\nFunctions:\nAPI Server for CLI/UI/SDK requests (authN/authZ via OIDC, RBAC checks per Herd) Barn (Cluster State Store under Raft consensus) for pipelines, Herd definitions, RBAC policies, and secrets Scheduler that matches pipeline tasks to available nodes, respecting resource quotas and Herd affinity Secrets Manager that securely stores and delivers credentials scoped to services and Herds Data Governance Service for lineage, quality, and annotation APIs High‚ÄëAvailability via stateless replicas and consensus‚Äëbacked state CLI Hub: Exposes commands (runi init, runi contract ‚Ä¶, runi compile, runi test, runi run, runi publish) for end‚Äëto‚Äëend pipeline management Project Scaffolding: Bootstraps new repositories with templates, configs, contracts, and CI/CD pipelines Feature \u0026amp; Contract Integration: Parses Feature DSL and schema contracts to drive code generation, testing, and execution Metadata Registry: Manages lineage, versioned contracts, run histories, and observability endpoints Plugin \u0026amp; Extension Manager: Loads and invokes user‚Äëprovided plugins, adapters, and custom steps With Herd at the helm, your platform is secure, consistent, and multi‚Äëtenant ready ‚Äî enforcing policies and providing a single source of truth for all pipelines and metadata. Every aspect of your data platform‚Äîfrom initial spec to production rollout‚Äîis standardized, traceable, and automatable, so teams can focus on delivering insight rather than wiring infrastructure.\nüíæ Barn (Cluster State Store) The Barn is the authoritative, highly available backbone of Runink‚Äôs control plane ‚Äî a distributed key-value store backed by Raft consensus. It provides a consistent, fault-tolerant source of truth for all runtime metadata, configurations, and orchestration logic across the platform.\nThis includes pipeline definitions, Herd boundaries, secrets metadata, RBAC policies, and scheduler state. Every component of the Runink platform ‚Äî from the API Server to the Scheduler and Governance Layer ‚Äî relies on this store to read, write, and replicate configuration and execution state. With strong consistency guarantees and built-in fault tolerance, the state store ensures that decisions made by the platform are safe, synchronized, and auditable.\nFunctions:\nPersist Herd definitions, RBAC policies, pipeline graphs, and secrets metadata Provide read/write access to the API Server, Scheduler, and Runi Agents Synchronize cluster-wide state using Raft-based consensus Support high-availability deployments with leader election and quorum Enable fault-tolerant execution of distributed pipelines and governance policies With the Barn, Runink gains a single source of truth that‚Äôs distributed by design ‚Äî making orchestration consistent, secure, and always in sync.\nüîê Secrets Manager The Secrets Manager is Runink‚Äôs secure vault for managing sensitive credentials, tokens, keys, and configuration values across environments and Herds. It ensures that secrets are stored safely, accessed securely, and delivered precisely when and where they\u0026rsquo;re needed ‚Äî with no manual handling required by pipeline authors.\nBacked by encrypted storage within the Barn, the Secrets Manager enforces strict per-Herd access controls, ensuring that only authorized agents and services can retrieve secrets for a given pipeline or slice. Secrets are injected just-in-time into ephemeral slice environments, avoiding persistence or leakage risks.\nWhether you‚Äôre integrating with databases, APIs, or cloud services, the Secrets Manager ensures that sensitive data stays encrypted, auditable, and access-controlled by default.\nFunctions:\nSecurely store secrets in an encrypted, Raft-backed data store Scope secret access using RBAC policies and Herd-level permissions Deliver secrets to Runi Agents on demand for authorized slices only Support dynamic secret injection into pipeline execution environments Ensure secrets are never written to disk or logs during runtime With the Secrets Manager, Runink provides seamless and secure secret management ‚Äî enabling safe access to credentials without compromising auditability or isolation.\nüîë Identity \u0026amp; RBAC Manager The Identity \u0026amp; RBAC Manager is the guardian of who can do what, where, and when across the Runink platform. It provides secure authentication and fine-grained, role-based authorization by linking users, service accounts, and slices to their permitted actions within specific Herds.\nIntegrated with OIDC providers (like Google, Okta, or GitHub), it supports Single Sign-On (SSO) and token-based identity. At the API layer, every request is checked against RBAC policies, ensuring only authorized users or agents can create pipelines, access secrets, or run code in a given domain.\nEach issued token (JWT) is short-lived, scoped to specific actions and Herds, and cryptographically verifiable ‚Äî enabling secure, auditable access across distributed components.\nFunctions:\nAuthenticate users and services via OIDC and JWT tokens Enforce fine-grained, Herd-scoped RBAC policies on all API and CLI actions Bind identities to service accounts, groups, or workload slices Issue, refresh, and revoke secure tokens with scoped permissions Audit access and permission changes across time and space With the Identity \u0026amp; RBAC Manager, Runink ensures secure, auditable, and granular access control ‚Äî empowering collaboration without compromising control.\nüñ•Ô∏è API Server The API Server is the main gateway to the Runink platform ‚Äî acting as the front door for developers, services, and tools. It exposes both REST and gRPC interfaces to support CLI commands (runink), SDKs, and any web-based UI clients, ensuring seamless access from local scripts to enterprise control planes.\nAll incoming requests pass through a robust authentication and authorization layer, powered by OIDC and RBAC policies, and are validated against the current cluster state. The API Server routes tasks, triggers executions, resolves Herd permissions, and coordinates interactions with internal services like the Scheduler, Governance layer, and Secrets Manager.\nWhether you\u0026rsquo;re registering a pipeline, validating a contract, or executing a scenario ‚Äî it all begins here.\nFunctions:\nAccept and route requests from CLI (runink), UI, SDKs, and APIs Authenticate and authorize users and agents based on OIDC and RBAC Validate inputs, resolve Herd scoping, and check quota policies Proxy commands to internal services (Scheduler, Governance, etc.) Serve both REST and gRPC endpoints for extensibility and automation With the API Server, Runink delivers a secure, scalable, and consistent interface for interacting with the entire data platform ‚Äî keeping pipelines and teams in sync.\nüìë Schema Contracts Schema Contracts are the cornerstone of Runink\u0026rsquo;s commitment to data integrity, trust, and evolution. They define the authoritative structure of your data ‚Äî including fields, types, formats, constraints, and relationships ‚Äî serving as a shared language between data producers, consumers, and pipeline logic.\nGenerated directly from Go structs, contracts are versioned, validated, and tracked through every stage of a pipeline. When upstream systems change, Runink detects schema drift, highlights diffs, and optionally blocks execution or routes data for quarantine ‚Äî ensuring nothing breaks silently. Contracts also tie directly into your tests, golden files, and metadata lineage, making them the connective tissue between your code, your data, and your governance. Runink treats contracts not just as schemas, but as data responsibility guarantees ‚Äî machine-verifiable, human-readable, and Git-versioned.\nFunctions:\nAuto-generate contracts from Go structs (runi contract gen) Detect and diff schema changes with drift alerts Enforce structural and semantic validation at ingest or transform Integrate with golden testing, feature execution, and lineage tracking Embed contract versions in audit logs, metadata, and observability signals With Schema Contracts, Runink gives your team confidence, control, and compliance ‚Äî by design.\nüß± Feature DSL (Domain-Specific Language) The Feature DSL is Runink‚Äôs powerful, readable interface for defining pipelines ‚Äî purpose-built to align data logic with domain language. Inspired by Gherkin, this DSL uses .dsl files to declare what should happen to the data, using a structured, declarative flow based on real business scenarios.\nEvery scenario is composed of modular @step blocks, organized using @when, @then, and @do to clearly express conditions, expected outcomes, and the logic to apply. These steps can be reused across domains ‚Äî for example, @step(name=NormalizeEmails) may appear in both ecommerce and finance pipelines, while more specific validations (like salary ranges or KYC checks) are scoped to their respective domain modules.\nThis structure makes it easy to translate business rules into testable pipelines, enabling close collaboration between technical and non-technical users ‚Äî with each scenario acting as both documentation and executable logic.\nFunctions:\nScenarios are composed of reusable, composable steps across domain contexts Each step uses @when ‚Üí @then ‚Üí @do semantics for clarity and modularity Supports tag-based step handlers (@source, @step(name=‚Ä¶), @sink) Domain-specific constraints and logic are cleanly isolated in modules DSL scenarios are fully testable, traceable, and convertible to pipeline code With the Feature DSL, Runink turns business scenarios into living, executable specifications ‚Äî bringing pipelines and domain intent closer together than ever before.\nüß™ Testing Engine Role: Ensures pipeline correctness and prevents regressions.\nThe Testing Engine is Runink‚Äôs built-in safety net ‚Äî ensuring that every pipeline transformation is correct, traceable, and regression-proof. At its heart is the golden file system, where each scenario‚Äôs expected output is saved and used for future comparisons. If anything changes during a test run, Runink shows a clear and structured diff, making it easy to spot regressions or confirm intentional updates.\nTests can be auto-generated using synthetic data, and pipelines can be validated in full using integration test suites. Whether you‚Äôre writing fine-grained unit tests or verifying complete feature flows, Runink leverages Go‚Äôs standard testing ecosystem alongside its own CLI to make pipeline testing reliable and repeatable.\nCombined with contracts, scenarios, and metadata tracking, this engine ensures that changes are safe, intentional, and observable ‚Äî without sacrificing developer speed.\nFunctions:\nGolden file testing with structured diff reporting Synthetic data generation for repeatable, contract-aligned tests CLI-driven integration testing with .dsl scenarios Catch schema drift, logic changes, or data regressions early All tests are human-readable and Git-friendly for review With Runink‚Äôs Testing Engine, quality isn‚Äôt bolted on ‚Äî it‚Äôs baked into every scenario, from the first line to the final output.\nüîç Interactive REPL: DataFrame / SQL Interface / JSON Explorer The Interactive REPL is your live terminal into the data universe ‚Äî purpose-built for exploration, debugging, and interactive prototyping. Whether you\u0026rsquo;re slicing through a JSON blob, cleaning up a CSV, or building a pipeline from scratch, the REPL gives you fast, expressive control over your data in real time.\nRunink‚Äôs REPL combines a DataFrame-inspired API, jq-style JSON navigation, and SQL-like querying into a single, cohesive shell. You can apply transformations step by step, inspect intermediate states, and convert your exploratory logic into reusable pipeline components ‚Äî all without rerunning an entire scenario or writing full Go code. It also supports live previews of how your SQL or REPL commands will map into structured Runink pipeline steps, helping bridge the gap between experimentation and production pipelines.\nFunctions:\nLive scenario prototyping and debugging, one step at a time JSON exploration with jq-style dot access and filtering Interactive DataFrame operations (Filter, Join, GroupBy, etc.) SQL-like query parsing for quick insights and structured filters Preview and export queries into .dsl pipeline DSL code Whether you‚Äôre writing tests, preparing contracts, or validating the latest stream of events ‚Äî the REPL keeps you curious, confident, and connected to your data.\nüöß Pipeline Generator The Pipeline Generator is the core of Runink‚Äôs automation engine ‚Äî turning human-readable .dsl files into optimized, production-ready Go pipelines. It analyzes each scenario and compiles it into a Directed Acyclic Graph (DAG) of transformations, ensuring that all steps are executed in the correct order while honoring their data dependencies.\nEach tag (@source, @step, @sink) in the DSL is parsed with context-aware parameters, allowing you to reuse logic across platforms and domains. Whether you\u0026rsquo;re sourcing from Snowflake, applying a standard validation like @step(name=ValidateSIN), or writing to a streaming sink, Runink generates tailored Go code behind the scenes to handle the orchestration.\nThe generator intelligently creates temporary views for intermediate pipeline steps and defines final outputs as durable tables ‚Äî giving you the flexibility to move between exploration and production without rewriting logic. Whether running in batch or real-time streaming mode, the generated pipelines are built with checkpointing, retries, and resilience out of the box.\nFunctions:\nCompiles .dsl DSL into fully executable Go code Builds DAGs (Directed Acyclic Graphs) from ordered step dependencies Supports parameterized tags like: @source(type=snowflake|kafka, schema|topic=‚Ä¶, table|format=‚Ä¶) @step(name=ValidateSIN, domain=‚Ä¶) @sink(type=snowflake|kafka, schema|topic=‚Ä¶, table|format=‚Ä¶) Generates views for intermediate pipeline objects Writes final outputs as persistent database tables Supports both batch and real-time streaming pipelines Includes built-in checkpointing, retries, and fault-tolerant flow control With the Pipeline Generator, you go from business intent to running code in seconds ‚Äî no YAML files, no glue code, just pure scenario-driven automation.\nüîÑ Feature Orchestration Role: Handles pipeline dependency resolution and execution scheduling.\nThe Feature Orchestration engine is the backbone of Runink‚Äôs intelligent pipeline execution. It transforms .dsl files into distributed DAGs (Directed Acyclic Graphs) that honor data dependencies, execution order, and domain boundaries. Each step ‚Äî tagged in your DSL ‚Äî is independently executable and can be scheduled as a microservice or serverless job, enabling true modular execution at scale.\nOrchestration works seamlessly across batch and streaming modes, supporting windowed ingestion and real-time event slices with equal efficiency. It leverages Go-native primitives (like bigmachine) and Linux cgroups to spin up lightweight, isolated Runi compute slices. These slices execute in logical groupings ‚Äî called Herds ‚Äî using Linux namespaces to separate domain-specific logic, making the entire system cloud-native and data-domain aware by design.\nWhether executing in a local cluster or across distributed environments like Kubernetes, Runink‚Äôs orchestration ensures resilience through retries, observability, auto-scaling, and minimal resource overhead ‚Äî giving you a serverless experience with full pipeline traceability and control.\nFunctions:\nRobust streaming and windowed processing, with batch ingestion support Parallel and distributed DAG-based pipeline execution Execution of tagged pipeline steps as microservices or serverless units Smart compute orchestration via Runi slices (cgroups) Domain boundary isolation using Herd namespaces Health checks, automatic retries, and self-healing execution plans Ingests both bounded and unbounded data with slice-based identity Matches pending pipeline tasks to available capacity. Reads desired tasks from the Barn Applies Herd quotas, node labels, and resource requirements Sends placement commands to Runi Agents With this architecture, Runink doesn‚Äôt just run pipelines ‚Äî it orchestrates data responsibility, compute efficiency, and modular autonomy at enterprise scale.\nüß≠ Data Lineage \u0026amp; Metadata The Data Lineage \u0026amp; Metadata layer in Runink gives you full transparency into how data flows ‚Äî and why it changes. As each record moves through a pipeline, Runink automatically tracks what transformed it, under what contract, when it happened, and where it ran. This built-in traceability turns every pipeline into an auditable, explorable map of data activity.\nRunink attaches rich metadata to every record ‚Äî including RunID, stage name, contract version, timestamps, and transformation paths ‚Äî all without requiring extra instrumentation. This metadata powers compliance reports, observability dashboards, and rollback logic. It also supports regulatory requirements like GDPR, HIPAA, and internal DataOps standards.\nVisual lineage graphs can be rendered from pipeline metadata to show how datasets were derived, making both debugging and documentation seamless.\nFunctions:\nAutomatic lineage tracking across all pipeline stages Visualize transformations via DAGs and lineage graphs Metadata tagging (RunID, stage, contract hash, timestamps) Immutable audit logs for compliance and traceability Hooks into REPL, test output, and contract validation reports With lineage and metadata as a first-class feature, Runink ensures your data is not just accurate, but accountable.\n‚úÖ Data Quality \u0026amp; Validation Role: Maintains data integrity and compliance throughout the pipeline.\nThe Data Quality \u0026amp; Validation layer in Runink ensures that your pipelines don‚Äôt just run ‚Äî they run with integrity, accuracy, and trust. It allows you to define validations as part of your pipeline logic, using both schema-driven rules and domain-specific assertions. From pre-ingestion contract validation checks to mid-pipeline schema enforcement validations and final output guarantees, Runink makes quality a first-class citizen.\nNon-technical users can define business rules using .dsl files ‚Äî asserting required fields, value ranges, regex patterns, or even cross-field relationships. Invalid records can be logged, routed to a Dead Letter Queue (DLQ), or flagged for review, without halting the entire pipeline.\nBuilt-in quality metrics and thresholds provide insight into pipeline health, and integrate seamlessly with observability tools for alerting and monitoring.\nFunctions:\nValidate records against schema contracts at multiple stages Enforce field-level, type-level, and domain-specific constraints Route invalid records to DLQs with full metadata for debugging Monitor and alert on data quality thresholds Allow business and data teams to codify validation rules in .dsl files With Runink, data quality becomes proactive, testable, and deeply integrated ‚Äî not an afterthought.\nüîí Data Governance \u0026amp; Compliance Role: Ensures adherence to regulatory and internal data governance standards.\nThe Data Governance \u0026amp; Compliance layer in Runink ensures that your pipelines don‚Äôt just move data ‚Äî they move it securely, transparently, and in compliance with both regulatory and internal standards. From role-based access control to encryption and masking, Runink provides the tools and policies to safeguard sensitive data while enabling responsible data sharing across domains.\nWhether you‚Äôre building pipelines for internal analytics or external-facing services, Runink enforces security best practices through clearly defined policies and supports key compliance frameworks like SOC 2, GDPR, and ISO 27001. Each component is designed to be traceable and accountable, with SPDX-compliant licensing, SBOM reporting, and secure identity handling via OIDC and JWT.\nData sensitivity is handled directly in the pipeline ‚Äî with built-in support for anonymization, data masking, and field-level encryption ‚Äî so governance becomes part of execution, not an afterthought.\nFunctions:\nExposes gRPC/REST API for UI, CLI, and policy checks Ingests annotations, metadata, and audit logs from slices Enables lineage visualization and quality dashboards SPDX-compliant licensing and SBOM (Software Bill of Materials) exports Integration and compliance with FDC3 and CDM standards Support for SOC 2, GDPR, ISO 27001, HIPAA, and more Schema alignment and data interoperability across financial systems Schema versioning and drift monitoring specific to industry models Applied FinOps on data Runink brings data responsibility and regulatory readiness directly into the pipeline, helping your organization scale securely ‚Äî and confidently.\nüõ°Ô∏è Security \u0026amp; Enterprise DataOps Role: Secure systems during its conception in an automated manner.\nThe Security \u0026amp; Enterprise DataOps layer in Runink ensures that your pipelines are secure from day one ‚Äî by automating protection across the entire software and data lifecycle. Security isn‚Äôt an afterthought; it‚Äôs integrated directly into how pipelines are built, tested, and deployed.\nRunink automates vulnerability scanning and license auditing during CI/CD, using GitHub Actions and GoReleaser to generate secure, signed artifacts with SBOMs and hash verification to prevent tampering. It supports environment-specific configurations, managing secrets and access per environment, whether local, staging, or production.\nFor runtime security, pipelines are governed by RBAC and authenticated via OIDC/JWT, while data is protected at multiple layers ‚Äî with encryption at rest and in transit, field-level masking, and anonymization for sensitive fields.\nFunctions:\nAutomated security scanning and vulnerability management during CI/CD Service Mesh integration Dynamic configuration management GitHub Actions + GoReleaser pipelines with reproducible builds Data encryption at rest (e.g., file systems, S3) and in transit (TLS) Environment-specific pipeline config + secret injection Fine-grained network access control and isolation Role-based access control (RBAC) with secure auth via OIDC/JWT Field-level encryption, masking, and anonymization for PII/PHI Signed hash verification of pipeline artifacts and outputs to prevent tampering Runink enables secure-by-default pipelines for modern, responsible data platforms ‚Äî empowering you to scale DataOps without sacrificing trust, traceability, or control. üåê Observability \u0026amp; Monitoring Role: Enables comprehensive visibility into pipeline health and performance.\nObservability means you can see what your pipelines are doing, when, and why. In Runink, every feature scenario and pipeline stage is fully instrumented with real-time metrics, structured logs, and traceable telemetry ‚Äî giving you deep, actionable visibility into your data workflows.\nFrom record counts and stage durations to error rates and schema contract versions, Runink captures detailed metrics tied to each run, stage, and data slice. These are exposed via Prometheus, OpenTelemetry, or your own observability stack, enabling rich dashboards, alerting, and deep debugging capabilities.\nStructured logs include tags like RunID, Stage, and ContractHash, while telemetry tracing shows full DAG execution across parallel or distributed nodes. Whether you\u0026rsquo;re tuning performance, chasing bugs, or watching for data anomalies ‚Äî Runink helps you stay one step ahead.\nFunctions:\nMonitor pipelines in real time with streaming metrics Debug slow or failing stages with stage-level performance data Alert on errors, retries, or threshold breaches Integrate data quality thresholds into observability dashboards Structured logs enriched with run metadata for traceability Visualize end-to-end pipeline execution with OpenTelemetry traces With Runink, observability is not an add-on ‚Äî it\u0026rsquo;s an embedded guarantee that your data platform stays transparent, reliable, and responsive. "
},
{
	"uri": "http://localhost:1313/contributing/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "ü§ù Contributing to Runink Welcome! First off, thank you for considering contributing to Runink. We deeply appreciate your support and effort to improve our project.\nThis document will guide you through the process of contributing code, filing issues, suggesting features, and participating in the Runink community.\nüìú Code of Conduct We expect everyone participating to adhere to our Code of Conduct (to be created). Respect and kindness are the foundation.\nüõ†Ô∏è How to Contribute 1. Fork the Repo Use GitHub\u0026rsquo;s \u0026ldquo;Fork\u0026rdquo; button to create a personal copy of the repository.\n2. Clone Your Fork git clone https://github.com/your-username/runink.git cd runink 3. Create a New Branch Use a clear branch naming convention:\ngit checkout -b feature/short-description # or git checkout -b fix/bug-description 4. Make Your Changes Follow our coding guidelines:\nWrite idiomatic Go (gofmt, golint). Keep PRs small and focused. Update or add tests for your changes. Update documentation (docs/) if applicable. 5. Test Before You Push Run all tests:\nmake lint make test 6. Push and Open a Pull Request Push to your fork and open a Pull Request against the main branch.\ngit push origin feature/short-description On GitHub, create a new Pull Request and fill in the template (title, description, related issues).\nüìã Development Guidelines CLI Commands: Place new commands inside their respective domain folder (barnctl, buildctl, herdctl, runictl). Testing: Add unit tests for CLI commands, helpers, validators. Logging: Use structured logging where needed. Security: Always consider security (no plaintext secrets, minimal privilege). Performance: Avoid premature optimization, but don\u0026rsquo;t introduce obvious inefficiencies. üîç Reporting Bugs Search existing issues first. File a new issue with clear reproduction steps. Provide logs, stack traces, and your environment (OS, Go version). If you discover a security vulnerability, please do not open a public issue.\nInstead, email us at paes@dashie.ink.\nüöÄ Suggesting Features Open an Issue labeled enhancement. Explain your use case and how it aligns with Runink\u0026rsquo;s vision. ‚ù§Ô∏è Code of Conduct We‚Äôre a community of data builders. We expect contributors to be respectful, inclusive, and constructive.\nPlease read our Code of Conduct before contributing.\nüßµ Join the Community GitHub Discussions (coming soon) Discord server (invite coming soon) Follow our roadmap in docs/roadmap.md üìÖ Regular Updates We sync main with active development regularly. Expect fast iteration and reviews.\nIf you have any questions, feel free to open an issue or discussion!\nThanks for being part of the Runink Herd and for helping us build the future of safe, expressive, and reliable data pipelines.üêë\nWe can‚Äôt wait to see what you contribute! üôå\n‚Äî The Runink Team\n"
},
{
	"uri": "http://localhost:1313/data-lineage/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Data Lineage \u0026amp; Metadata Tracking ‚Äì Runink Runink pipelines are designed to be fully traceable, auditable, and schema-aware. With built-in lineage support, every pipeline can generate:\nVisual DAGs of data flow and dependencies Metadata snapshots with schema versions and field hashes Run-level logs for audit, debugging, and compliance This guide walks through how Runink enables robust data observability and governance by default.\nüîç What Is Data Lineage? Lineage describes where your data came from, what happened to it, and where it went.\nIn Runink, every pipeline run captures:\nSources: file paths, streaming URIs, tags Stages: steps applied, transform versions Contracts: schema file, struct, and hash Sinks: output paths, filters, conditions Run metadata: timestamps, roles, record count üìà Generate a Lineage Graph runi lineage --scenario features/orders.dsl --out lineage/orders.svg The graph shows:\nInputs and outputs All applied steps Contract versions and field diff hashes Optional labels (e.g., role, source, drift) üßæ Per-Run Metadata Log Every run emits a record like:\n{ \u0026#34;run_id\u0026#34;: \u0026#34;run-20240423-abc123\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;JoinUsersAndOrders\u0026#34;, \u0026#34;contract\u0026#34;: \u0026#34;user_order_v2.json\u0026#34;, \u0026#34;schema_hash\u0026#34;: \u0026#34;b72cd1a\u0026#34;, \u0026#34;records_processed\u0026#34;: 9123, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-04-23T11:02:00Z\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;analytics\u0026#34;, \u0026#34;drift_detected\u0026#34;: false } üß™ Snapshotting \u0026amp; Version Tracking You can snapshot inputs/outputs with:\nruni snapshot --contract contracts/user.json --out snapshots/users_2024-04-23.json And later compare against historical output.\nüö® Drift Detection Runink detects when incoming data deviates from expected contract:\nruni contract diff --old v1.json --new incoming.json Or as part of a scenario run:\nruni run --verify-contract This flags:\nMissing/extra fields Type mismatches Tag mismatches (e.g., missing pii, access) üîê Metadata for Compliance Attach metadata to every stage:\ntype StageMetadata struct { RunID string Role string Contract string Hash string Source string Timestamp string } Send this to a:\nDocument DB (e.g. Mongo) Data lake (e.g. MinIO, S3) Audit stream (e.g. Kafka topic) üì° Monitoring \u0026amp; Observability Runink supports Prometheus metrics per stage:\nruni_records_processed_total runi_stage_duration_seconds runi_schema_drift_detected_total runi_invalid_records_total üß† Example Use Cases Role How Lineage Helps Data Engineer Debug broken joins, drift, formats Analyst Understand where numbers came from Governance Prove schema conformance ML Engineer Snapshot training input lineage Summary Runink provides end-to-end data lineage as a first-class feature, not an afterthought:\nBuilt-in visual DAGs Contract + transform metadata Auditable, role-aware stage outputs Real-time observability with metrics Lineage lets you move fast without breaking trust.\n"
},
{
	"uri": "http://localhost:1313/feature-dsl/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Advanced Feature DSL Syntax ‚Äì Runink Runink uses .dsl files with a human-friendly DSL (Domain-Specific Language) inspired by BDD and Gherkin. These files define declarative data pipelines using structured steps, annotations, and contract references.\nThis guide explores advanced syntax available for real-world data use cases including streaming, branching, conditionals, role enforcement, and metadata tagging.\nüìå Anatomy of a .dsl File Feature: High-value customer segmentation @module(layer=\u0026#34;bronze\u0026#34;, domain=) Scenario: Normalize Bronze Layer Product Data @source(\u0026#34;json://testdata/input/products_feed.json\u0026#34;) @step(\u0026#34;AddIngestMetadata\u0026#34;) @step(\u0026#34;TagMissingFields\u0026#34;) @sink(\u0026#34;json://bronze/products_bronze.json\u0026#34;) Given the contract: contracts/products_raw.json When data is received from the vendor feed Then the ingestion timestamp should be added And missing required fields (sku, name, price) should be tagged Do log metadata for every record --- ## Scenario: Silver Layer - Normalize and Standardize @module(layer=\u0026#34;silver\u0026#34;, domain=) Scenario: Standardize Product Schema @source(\u0026#34;json://bronze/products_bronze.json\u0026#34;) @step(\u0026#34;TrimProductNames\u0026#34;) @step(\u0026#34;StandardizeCurrency\u0026#34;) @step(\u0026#34;FixEmptyDescriptions\u0026#34;) @sink(\u0026#34;json://silver/products_silver.json\u0026#34;) Given the contract: contracts/products_normalized.json When records contain inconsistent formatting Then product names should be trimmed And currencies should default to USD if missing And empty descriptions replaced with a default message Do emit standardized and validated output --- ## Scenario: Gold Layer - Curate for Analytics \u0026amp; Governance @module(layer=\u0026#34;gold\u0026#34;, domain=) Scenario: Enrich and Curate Product Catalog @source(\u0026#34;json://silver/products_silver.json\u0026#34;) @step(\u0026#34;GroupVariantsByFamily\u0026#34;) @step(\u0026#34;EnrichWithCategoryLTV\u0026#34;) @step(\u0026#34;DetectDiscontinuedItems\u0026#34;) @sink(\u0026#34;json://gold/products_curated.json\u0026#34;) Given the contract: contracts/products_curated.json When normalized product data is ready Then variants should be grouped by SKU family And categories should have a calculated LTV score And discontinued items should be flagged by description Do finalize product output with metadata for BI usage üîÅ Iterative Scenarios Use multiple Scenario blocks per pipeline variant:\nScenario: Flag suspicious orders Scenario: Apply loyalty points üîÑ Streaming with Windowing @source(\u0026#34;kafka://events.orders\u0026#34;, @window(5m)) Windowing modes:\n@window(5m) ‚Äî Tumbling @window(sliding:10m, every:2m) ‚Äî Sliding @window(session:15m) ‚Äî Session ‚ö†Ô∏è Conditional Routing Send records to sinks based on dynamic conditions:\n@sink(\u0026#34;json://clean/users.json\u0026#34; when \u0026#34;user.active == true\u0026#34;) @sink(\u0026#34;json://dlq/inactive.json\u0026#34; when \u0026#34;user.active == false\u0026#34;) üîê Role-based Output @sink(\u0026#34;json://ops_view.json\u0026#34; with \u0026#34;role=ops\u0026#34;) @sink(\u0026#34;json://finance_view.json\u0026#34; with \u0026#34;role=finance\u0026#34;) Paired with access:\u0026quot;role\u0026quot; in contracts.\nüß™ Test Hooks @golden(\u0026#34;testdata/orders.golden.json\u0026#34;) @assert(\u0026#34;records == 100\u0026#34;) üìé Source Metadata Tags @source(\u0026#34;csv://products.csv\u0026#34; @meta(region=\u0026#34;us\u0026#34;, vendor=\u0026#34;x\u0026#34;)) This metadata is passed into transforms and lineage.\nüß¨ Reusing Steps (Registry) Steps like NormalizeEmail, FilterInactiveUsers, TagLTV can be reused across scenarios with no code duplication.\nüß∞ Combining Sources @source(\u0026#34;csv://orders.csv\u0026#34;) @source(\u0026#34;json://users.json\u0026#34;) @step(\u0026#34;JoinOrdersAndUsers\u0026#34;) Each record is tagged with source path in the pipeline.\nüßπ Side Effects and Emissions @emits(\u0026#34;alerts/vip_discovered\u0026#34;) @step(\u0026#34;SendWebhook\u0026#34;) Triggers external systems while running pipelines.\nüß† LLM-based Annotations Use AI-generated transform suggestions via:\n@auto(\u0026#34;summarize user activity\u0026#34;) Runink can then generate or recommend pipeline stages for the task.\nSummary Runink‚Äôs .dsl DSL lets you:\nDescribe pipelines in natural, reusable syntax Build complex branching and streaming workflows Embed contracts, policies, and roles into your ETL It‚Äôs not just testable ‚Äî it‚Äôs self-documenting, composable, and production-ready.\n"
},
{
	"uri": "http://localhost:1313/getting_started/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Getting Started with Runink Welcome to Runink! This quick-start guide will help you get up and running with Runink to effortlessly build, test, and run data pipelines.\nüöÄ 1. Installation Make sure you have Go installed (v1.20 or later). Then install Runink:\ngo install github.com/runink/runink@latest Ensure $GOPATH/bin is in your $PATH.\nüõ† 2. Initialize Your Project Create a new Runink project in seconds:\nrunink init myproject cd myproject This command generates:\nInitial Go module Sample contracts Example .dsl files Golden file tests Dockerfile and CI/CD configs üìã 3. Explore the Project Structure Your project includes:\nmyproject/ ‚îú‚îÄ‚îÄ bin/ -\u0026gt; CLI ‚îú‚îÄ‚îÄ api/ -\u0026gt; gRPC API layer (authn/z, herd context) ‚îú‚îÄ‚îÄ contracts/ -\u0026gt; Schema contracts and transformation logic on go struts. ‚îú‚îÄ‚îÄ features/ -\u0026gt; Scenarios definitions for each feature from the `.dsl` files. ‚îú‚îÄ‚îÄ golden/ -\u0026gt; Golden files used on regression testing with examples and synthetic data. ‚îú‚îÄ‚îÄ dags/ -\u0026gt; Generated DAG code from the contracts and features to be executed by runi. ‚îú‚îÄ‚îÄ herd/ -\u0026gt; Domain Service Control Policies Herd context isolation ‚îú‚îÄ‚îÄ docs/ -\u0026gt; Markdown docs, examples, use cases, and playbooks. ‚îú‚îÄ‚îÄ config/runi/ -\u0026gt; Agent: runner, cgroup manager, metrics, logs ‚îú‚îÄ‚îÄ config/raftstore/ -\u0026gt; State sync using Raft ‚îú‚îÄ‚îÄ config/scheduler/ -\u0026gt; DAG-aware scheduler logic ‚îú‚îÄ‚îÄ config/parser/ -\u0026gt; DSL parser, scenario runner ‚îú‚îÄ‚îÄ config/observability/ -\u0026gt; Tracing, logging, Prometheus exporters ‚îî‚îÄ‚îÄ .github/workflows/ ‚öôÔ∏è 4. Compile and Run Pipelines Compile your first pipeline:\nrunink compile --scenario features/example.dsl --out pipeline/example.go --herd my-data-herd Execute a scenario:\nrunink run --scenario features/example.dsl --herd my-data-herd ‚úÖ 5. Test Your Pipelines Use built-in testing and golden files to ensure correctness:\nrunink test --scenario features/example.dsl --herd my-data-herd If the pipeline logic changes and the test is intentionally updated, regenerate golden files:\nrunink test --scenario features/example.dsl --update --herd my-data-herd üîç 6. Interactive REPL Interactively explore data and debug transformations:\nrunink repl --scenario features/example.dsl --herd my-data-herd Example REPL commands:\nload csv://data/input.csv apply MyTransform show üìö 7. Next Steps Explore advanced feature DSL syntax Learn about data lineage and metadata tracking Understand schema and contract management üöß Support \u0026amp; Community Need help or have suggestions?\nOpen an issue on GitHub Join our community discussions and get involved! Let\u0026rsquo;s start building amazing data pipelines with Runink!\n"
},
{
	"uri": "http://localhost:1313/glossary/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Runink Glossary This glossary defines key terms, acronyms, and concepts used throughout the Runink documentation and codebase.\n.dsl File A human-readable file written in Gherkin syntax used to describe a data pipeline scenario using Given/When/Then structure and tags like @source, @step, and @sink.\nBDD (Behavior-Driven Development) A software development approach that describes application behavior in plain language, often used with .dsl files.\nGolden File A snapshot of the expected output from a pipeline or transformation, used to assert correctness in automated tests.\nSchema Contract A versioned definition of a data structure (e.g., JSON, Protobuf, Go struct) used to validate pipeline input/output and detect schema drift.\nSchema Drift An unintended or unexpected change in a schema that may break pipeline compatibility.\nDAG (Directed Acyclic Graph) A graph of pipeline stages where each edge represents a dependency, and there are no cycles. Used for orchestrating non-linear workflows.\nDLQ (Dead Letter Queue) A place to store invalid or failed records so they can be analyzed and retried later without interrupting the rest of the pipeline.\nREPL (Read-Eval-Print Loop) An interactive interface that lets users type commands and immediately see the results. Runink‚Äôs REPL supports loading data, applying steps, and viewing outputs.\nLineage A traceable path showing how data flows through each pipeline stage, from source to sink, including what transformed it and which contract was applied.\nPrometheus A monitoring system used to collect and store metrics from pipeline executions.\nOpenTelemetry An observability framework for collecting traces and metrics, helping to visualize the execution path and performance of pipelines.\ngRPC A high-performance, open-source universal RPC framework used for running distributed pipeline stages.\nProtobuf (Protocol Buffers) A method for serializing structured data, used in gRPC communication and schema definitions.\nBigmachine A Go library for orchestrating distributed, stateless workers. Used by Runink to scale pipelines across multiple machines.\nContract Hash A hash value generated from a schema contract to uniquely identify its version. Used for detecting changes and tracking usage.\nSBOM (Software Bill of Materials) A manifest of all dependencies and components included in a software release, used for compliance and security auditing.\nFDC3 (Financial Desktop Connectivity and Collaboration Consortium) A standard for interop between financial applications. Runink can integrate with FDC3 schemas and messaging models.\nCDM (Common Domain Model) A standardized schema used in finance and trading to represent products, trades, and events. Supported natively by Runink.\nHave a term you‚Äôd like added? Open an issue or suggest a change in the docs!\n"
},
{
	"uri": "http://localhost:1313/roadmap/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "üó∫Ô∏è Runink Roadmap Welcome to the official Runink Roadmap ‚Äî our evolving guide to what we\u0026rsquo;re building, where we‚Äôre headed, and how you can get involved.\nRunink is built on the belief that modern data platforms should be safe by default, composable by design, and collaborative at scale. This roadmap reflects our commitment to transparency, community-driven development, and rapid iteration.\nüß© Roadmap Themes Theme Description Composable Pipelines Make it easy to build, reuse, and test pipeline steps across teams and domains. Secure \u0026amp; Compliant by Default Tighten RBAC, data contracts, and observability for enterprise-grade governance. DevX \u0026amp; Developer Productivity Empower devs with a powerful CLI, REPL, codegen, and rapid iteration loops. Streaming-First DataOps Advance real-time use cases with backpressure-safe, contract-aware streaming. Interoperability \u0026amp; Ecosystem Play well with FDC3, CDM, OpenLineage, Kafka, Snowflake, and more. üß≠ Current Focus (Q2 2025) These items are in active development or early testing:\nHerd Namespace Isolation (multi-tenant namespace support) Golden Test Rewrites for easier review and diffing CLI REPL SQL Mode with DataFrame-to-Feature export RBAC \u0026amp; Token Scoping Enhancements Raft-backed Barn \u0026amp; Secrets Manager Integration gRPC Streaming Orchestration Pipeline Preview Mode (dry-run with metadata only) Lineage UI + CLI support Remote Artifact Signing \u0026amp; SBOM generation (SLSA-style) üîú Near-Term (Q3 2025) Planned next based on user feedback and enterprise needs:\nLive Feature File Linter \u0026amp; Formatter REPL Session Recorder (record ‚Üí replay feature building) Multi-Herd Scheduling \u0026amp; Cost Reporting Secrets Rotation + External Vault Integration Contract Diff Web Viewer Push-to-Registry UX from CLI DLQ Visualization + Retry Tools Plugin Marketplace (source/sink/step handlers) üåÖ Long-Term Vision (Late 2025+) Our long-range goals to shape Runink into the standard platform for responsible data pipelines:\n‚öôÔ∏è Full No-YAML Orchestration (Declarative-Only Pipelines) üß† AI Copilot for Contract \u0026amp; Scenario Generation üåê Cross-Org Data Mesh Support via Herd Federation üì° Runink Cloud (fully managed, secure, multi-tenant SaaS) üîí Zero-Trust Data Contracts (ZK + Provenance) üß† Ideas We\u0026rsquo;re Exploring These are in research/design phases ‚Äî feedback welcome!\n‚ú® Feature DSL Step Suggestions in CLI üîÄ Schema Merge Conflict Resolution UX üì• Native ingestion support for S3/Parquet/Arrow üîé Full integration with OpenLineage + dbt Core üßæ GitHub Copilot integration for contract authoring üôã‚Äç‚ôÄÔ∏è Contribute to the Roadmap We prioritize what the community and users need most. If there‚Äôs a feature you‚Äôd love to see:\nOpen an issue using the Feature Request template Upvote existing roadmap items via üëç reactions Join upcoming roadmap discussions (Discord coming soon!) PRs welcome for anything marked as help-wanted üîÑ Release Cadence We aim for:\nMinor releases every 4‚Äì6 weeks (feature drops, improvements) Patch releases as needed (hotfixes, regressions) Major milestones every ~6 months with community showcases Track progress in CHANGELOG.md\nThanks for being part of the journey ‚Äî we‚Äôre building Runink with you, not just for you. Let‚Äôs define the future of safe, modular, and explainable data platforms together.\n‚Äî Team Runink üêë\n"
},
{
	"uri": "http://localhost:1313/runink_quickstart/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "üöÄ Runink Quickstart: CDM Trade Pipeline This example shows how to define, test, apply, and run a declarative data pipeline using Runink.\nüõ†Ô∏è Prerequisites Ensure you have:\nA .dsl scenario: features/cdm_trade/trade_cdm.dsl A Go contract file: contracts/trade_cdm_multi.go Golden test files: golden/cdm_trade/ Sample input data: golden/cdm_trade/input.json Our example presents the following:\nKafka (raw) ‚Üì DecodeCDMEvents ‚Üí sf://control.decoded_cdm_events ‚Üì ValidateLifecycle ‚îú‚îÄ‚Üí sf://control.invalid_cdm_events (if !valid) ‚îî‚îÄ‚Üí TagWithFDC3Context ‚Üì sf://cdm.validated_trade_events üí° Example Flow # Create a secure namespace (herd) runi herd create finance # Run the pipeline in test mode against golden test files runi test \\ --scenario features/cdm_trade/cdm_trade.dsl \\ --golden golden/cdm_trade \\ --herd finance # Compile + apply the DAG runi apply \\ --scenario features/trade_cdm.dsl \\ --contract contracts/cdm_trade/trade_cdm_multi.go \\ --herd finance \\ --out dags/cdm_trade/trade_cdm_dag.go # Execute the pipeline with test input runi run \\ --scenario features/cdm_trade/trade_cdm.dsl \\ --herd finance \\ --input golden/cdm_trade/input.json üìä Inspect Pipeline Execution After running, inspect the pipeline using:\nruni status --run-id RUN-20240424-XYZ --herd finance Example Output run_id: RUN-20240424-XYZ herd: finance status: completed steps: - DecodeCDMEvents: processed: 2 output: sf://control.decoded_cdm_events - ValidateLifecycle: passed: 1 failed: 1 output: - valid ‚Üí sf://cdm.validated_trade_events - invalid ‚Üí sf://control.invalid_cdm_events - TagWithFDC3Context: enriched: 1 context_prefix: fdc3.instrumentView: lineage: contract_hash: a9cd23f‚Ä¶ contract_version: v3 created_by: service-account:etl-runner üîç Follow-Up Commands runi lineage --run-id RUN-20240424-XYZ runi logs --run-id RUN-20240424-XYZ runi publish --herd finance --scenario features/cdm_trade/cdm_trade.dsl Runink makes secure, declarative data orchestration easy ‚Äî every pipeline is testable, auditable, and reproducible.\n"
},
{
	"uri": "http://localhost:1313/schema-contracts/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Schema \u0026amp; Contract Management ‚Äì Runink Runink enables data contracts as native Go structs ‚Äî giving you strong typing, version tracking, schema validation, and backward compatibility across pipelines.\nThis guide shows how to define, version, test, and enforce schema contracts in your pipelines.\nüì¶ What Is a Contract? A contract in Runink is a schema definition used to:\nValidate incoming and outgoing data Detect schema drift Provide PII and RBAC tagging Drive pipeline generation and testing Contracts are generated from Go structs annotated with tags.\n‚úçÔ∏è Defining a Contract package contracts type Order struct { OrderID string `json:\u0026#34;order_id\u0026#34;` CustomerID string `json:\u0026#34;customer_id\u0026#34;` Amount float64 `json:\u0026#34;amount\u0026#34;` Timestamp string `json:\u0026#34;timestamp\u0026#34;` Notes string `json:\u0026#34;notes\u0026#34; pii:\u0026#34;true\u0026#34; access:\u0026#34;support\u0026#34;` } Then run:\nruni contract gen --struct contracts.Order --out contracts/order.json ‚úÖ Enforcing a Contract Given the contract: contracts/order.json Or:\nruni run --verify-contract Runink ensures that all records match the expected schema.\nüîç Schema Drift Detection Compare current vs expected schema:\nruni contract diff --old v1.json --new v2.json Output shows added, removed, or changed fields, types, tags, and ordering.\nüìä Hashing and Snapshotting Each contract has a hash for:\nVersion tracking Lineage graph integrity Change detection runi contract hash contracts/order.json Snapshot for reproducibility:\nruni snapshot --contract contracts/order.json --out snapshots/order_v1.json üß¨ Advanced Tags pii:\u0026quot;true\u0026quot; ‚Äì marks field as sensitive access:\u0026quot;finance\u0026quot; ‚Äì restricts field to roles enum:\u0026quot;pending,approved,rejected\u0026quot; ‚Äì enum constraint (optional) required:\u0026quot;true\u0026quot; ‚Äì fail if field is null or missing üß™ Contract Testing Use golden tests to assert schema correctness:\nruni test --scenario features/orders.dsl And diff output against expected:\nruni diff --gold testdata/orders.golden.json --new out/orders.json üóÉÔ∏è Contract Catalog Generate an index of all contracts in your repo:\nruni contract catalog --out docs/contracts.json This can be plugged into:\nDocs browser Contract registry CI schema check üßæ Example Contract Output { \u0026#34;name\u0026#34;: \u0026#34;Order\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;order_id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;float64\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;notes\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pii\u0026#34;: true, \u0026#34;access\u0026#34;: \u0026#34;support\u0026#34; } ], \u0026#34;hash\u0026#34;: \u0026#34;a94f3bc...\u0026#34; } Summary Contracts in Runink power everything:\nSchema validation RBAC and compliance Pipeline generation Test automation Lineage and snapshots Use contracts to make your data:\nSafe Trustworthy Documented Governed "
},
{
	"uri": "http://localhost:1313/security/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Supported Versions We currently support the latest major release of pipetool. Older versions may not receive security updates or patches.\nReporting a Vulnerability If you discover a security vulnerability, please do not open a public issue.\nInstead, contact us directly:\nEmail: security@yourdomain.org PGP Key: https://yourdomain.org/pgp.key (optional) We aim to respond to all security reports within 5 business days. All disclosures will be handled confidentially and professionally.\nDisclosure Process Vulnerability reported via email Maintainers investigate and validate the issue A patch is prepared and tested privately Coordinated disclosure timeline is agreed upon with reporter Advisory + patched release are published Hall of Fame We may credit contributors who report valid vulnerabilities in our release notes, changelogs, or SECURITY.md ‚Äî with consent.\nThank you for helping make Runink safer for everyone!\n"
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/",
	"title": "Runink Project",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]